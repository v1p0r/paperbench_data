[
    {
        "type": "text",
        "text": "LCA-on-the-Line: Benchmarking Out-of-Distribution Generalization with Class Taxonomies ",
        "text_level": 1,
        "bbox": [
            217,
            109,
            756,
            154
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Jia Shi 1 Gautam Gare 1 Jinjin Tian 1 Siqi Chai 1 Zhiqiu Lin 1 Arun Vasudevan 1 Di Feng 2 3 Francesco Ferroni 2 4 Shu Kong 5 6 ",
        "bbox": [
            148,
            195,
            820,
            227
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Abstract ",
        "text_level": 1,
        "bbox": [
            243,
            253,
            318,
            270
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "We tackle the challenge of predicting models’ Out-of-Distribution (OOD) performance using indistribution (ID) measurements without requiring OOD data. Existing evaluations with “Effective Robustness”, which use ID accuracy as an indicator of OOD accuracy, encounter limitations when models are trained with diverse supervision and distributions, such as class labels (Vision Models, VMs, on ImageNet) and textual descriptions (Visual-Language Models, VLMs, on LAION). VLMs often generalize better to OOD data than VMs despite having similar or lower ID performance. To improve the prediction of models’ OOD performance from ID measurements, we introduce the Lowest Common Ancestor (LCA)- on-the-Line framework. This approach revisits the established concept of LCA distance, which measures the hierarchical distance between labels and predictions within a predefined class hierarchy, such as WordNet. We assess 75 models using ImageNet as the ID dataset and five significantly shifted OOD variants, uncovering a strong linear correlation between ID LCA distance and OOD top-1 accuracy. Our method provides a compelling alternative for understanding why VLMs tend to generalize better. Additionally, we propose a technique to construct a taxonomic hierarchy on any dataset using $K$ -means clustering, demonstrating that LCA distance is robust to the constructed taxonomic hierarchy. Moreover, we demonstrate that aligning model predictions with class taxonomies, through soft labels or prompt engineering, can enhance model generalization. Open source code in our Project Page. ",
        "bbox": [
            120,
            279,
            442,
            791
        ],
        "page_idx": 0
    },
    {
        "type": "image",
        "img_path": "images/d2ed7dd47bfdba9a1aaf4cfa79051e690e3347e8361ab1d83790d1ce1254a849.jpg",
        "image_caption": [
            "Figure 1. Correlation between LCA distance and out-ofdistribution (OOD) performance in Vision and VisionLanguage Models (VLMs). In both panels, the $X -$ -axis represents the top-1 accuracy on ObjectNet (OOD test dataset). The Y-axes depict the top-1 accuracy (left-axis) and LCA distance (right-axis) on ImageNet (ID test dataset). The left plot reveals a divergent trend where Vision Models (VMs) show a trade-off between OOD and ID accuracy, while VLMs tend to maintain higher OOD accuracy regardless of ID performance. The right plot demonstrates a unified, strong positive correlation between LCA distance and OOD accuracy for both VMs and VLMs, showing that LCA distance is a robust metric for evaluating model generalization across different architectures, model modalities, and training data sources. "
        ],
        "image_footnote": [],
        "bbox": [
            501,
            251,
            883,
            429
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "1. Introduction ",
        "text_level": 1,
        "bbox": [
            501,
            667,
            625,
            684
        ],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Generalizing models trained on in-distribution (ID) data to out-of-distribution (OOD) conditions is a notoriously difficult task. Distribution shifts undermine the independent and identically distributed (IID) assumption between training and testing data, challenging the model’s robustness. Numerous OOD datasets have been proposed to study the effects of different interventions, such as temporal shifts (Hu et al., 2022; Lomonaco & Maltoni, 2017; Lin et al., 2021), artificial noise (Hendrycks & Dietterich, 2019; Arjovsky et al., 2019; Larochelle et al., 2008), and natural distribution shifts (Hendrycks et al., 2021; Hendrycks & Dietterich, 2019; Barbu et al., 2019; Recht et al., 2019). Maintaining model robustness becomes significantly more difficult with severe visual shifts in the image domain. However, many studies evaluate generalization on OOD datasets with limited visual shifts or only involve artificial noise, such as ImageNet-v2 or ImageNet-C (Recht et al., 2019; Arjovsky et al., 2019). Such datasets fail to fully reflect a model’s generalization capability when confronted with severe distribution shifts (Hendrycks et al., 2021; Hendrycks & Dietterich, 2019; Barbu et al., 2019), as there is often limited transfer of robustness from synthetic to natural distribution shifts (Taori et al., 2020). ",
        "bbox": [
            501,
            695,
            885,
            904
        ],
        "page_idx": 0
    },
    {
        "type": "discarded",
        "text": "1Carnegie Mellon University 2Work done at Argo AI GmbH 3Now at Apple $^ { 4 } \\mathrm { { N o w } }$ at Nvidia 5Texas A&M University 6University of Macau. Correspondence to: Jia Shi <jiashi $@$ alumni.cmu.edu>, Shu Kong <skong@um.edu.mo>. ",
        "bbox": [
            88,
            803,
            473,
            854
        ],
        "page_idx": 0
    },
    {
        "type": "discarded",
        "text": "Proceedings of the $\\mathit { 4 1 } ^ { s t }$ International Conference on Machine Learning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by the author(s). ",
        "bbox": [
            88,
            866,
            472,
            905
        ],
        "page_idx": 0
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            22,
            273,
            62,
            709
        ],
        "page_idx": 0
    },
    {
        "type": "discarded",
        "text": "1 ",
        "bbox": [
            482,
            922,
            490,
            934
        ],
        "page_idx": 0
    },
    {
        "type": "image",
        "img_path": "images/2f627c520cbb371e4d703ca1d0b31fdef4295788dc8862579ab9f19235cee4c5.jpg",
        "image_caption": [
            "Figure 2. Comparison of our setting with prior work. Left: prior work settings such as Accuracy-on-the-line (Miller et al., 2021) and Agreement-on-the-line (Baek et al., 2022). Right: our setting. To the best of our knowledge, LCA-on-the-line is the first approach to uniformly measure model robustness across VMs and VLMs on OOD datasets with significant distribution shifts (ImageNet-S/R/A/O). "
        ],
        "image_footnote": [],
        "bbox": [
            96,
            77,
            879,
            273
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            89,
            347,
            473,
            482
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "In the realm of model generalization, numerous attempts have been made to predict a model’s performance on OOD datasets based on in-distribution measurements, following the concept of effective robustness (Taori et al., 2020). These approaches, referred to as $\\mathbf { \\epsilon } ^ { \\bullet } \\mathbf { X }$ -on-the-line’ (Miller et al., 2021; Baek et al., 2022), suggest that a model’s OOD performance is correlated to in-distribution accuracy (Miller et al., 2021; Recht et al., 2019; Miller et al., 2020; Roelofs et al., 2019) or models consensus on in-distribution accuracy (Jiang et al., 2021; Baek et al., 2022). ",
        "bbox": [
            89,
            489,
            473,
            640
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Moreover, several prior attempts rely on domain generalization strategies that necessitate prior knowledge of the target domain or require an estimation of OOD domain information (Chen et al., 2021; Li et al., 2022a). These can lead to computationally intensive processes, particularly when involving multiple models or inferences (Baek et al., 2022; Deng et al., 2022). ",
        "bbox": [
            89,
            648,
            473,
            753
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Most prior research has focused solely on estimating generalization among vision models (VMs) supervised on class labels trained on ImageNet (Taori et al., 2020; Mustafa et al., 2020). Emerging large-scale Vision-Language Models (VLMs) trained on datasets like LAION demonstrate exceptional generalization performance on out-of-distribution (OOD) data. However, as shown on the left plot of Fig. 1, existing evaluation (Miller et al., 2021) using ID accuracy fail to explain the effective robustness (Taori et al., 2020) gap between VMs and VLMs. This underscores the necessity to evaluate and compare models across different families under a unified evaluation framework. Recently, (Shi et al., 2023) observed the same problem and proposed evaluating OOD accuracy using multiple ID test sets, but their method requires multiple evaluation runs. ",
        "bbox": [
            89,
            762,
            473,
            897
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            501,
            347,
            885,
            436
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Unlike VMs, VLMs leverage more diverse training data, contrastive loss, and language supervision. There have been attempts to measure VLM generalization (HaoChen et al., 2021; Fang et al., 2022; Schuhmann et al., 2022; Kaur et al., 2022), specifically suggesting that diversity in training data is an indicator of model generalization. However, it is nontrivial to measure data diversity, and even collect and train on such large-scale diverse data (Schuhmann et al., 2022). ",
        "bbox": [
            501,
            444,
            885,
            565
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Prior attempts lack a unified, simple measurement for both VMs and VLMs to explain model generalization and convert it into actionable improvements. To address the issues of (1) lack of unified metrics for VLMs and VMs, or models trained on different data sources; (2) need for robustness to large domain shifts; (3) desire for computationally efficient metrics, we propose adopting the Lowest Common Ancestor (LCA) distance to measure model generalization. The LCA distance is the taxonomic distance between labels and predictions, given a predefined class hierarchy, such as WordNet. Through a series of empirical experiments involving 75 models (36 VMs and 39 VLMs) (cf. Fig. 2), we show that the in-distribution LCA distance strongly correlates with multiple ImageNet-OOD datasets under severe visual shifts (cf. Fig. 1 right plot). This finding may help explain the surprising result that zero-shot vision-language models with poor top-1 accuracy generalize better to novel datasets compared to state-of-the-art vision models. This spurs us to further investigate and discuss the potential of the LCA benchmark for improving model generalization. We also discuss the suitability of LCA as a generalization indicator in Section 3. ",
        "bbox": [
            500,
            573,
            885,
            904
        ],
        "page_idx": 1
    },
    {
        "type": "discarded",
        "text": "2 ",
        "bbox": [
            482,
            922,
            491,
            934
        ],
        "page_idx": 1
    },
    {
        "type": "discarded",
        "text": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies ",
        "bbox": [
            187,
            58,
            779,
            69
        ],
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "In summary, we make the following major contributions: (1) We propose the Lowest Common Ancestor (LCA) distance as a new metric for evaluating model generalization. This benchmark utilizes class hierarchies, such as WordNet, which encode relationships between classes. (2) We validate our benchmarking strategy through large-scale experiments, analyzing 75 models across five ImageNet-OOD datasets. Our findings reveal a strong linear correlation between in-distribution LCA and OOD Top-1 performance, thus establishing the ‘LCA-on-the-Line’ framework. (3) We offer a thorough analysis of the connection between LCA and model generalization, providing new insights to inspire further research in this area. (4) For datasets without a predefined hierarchy, we introduce a method for constructing latent hierarchies using K-means clustering. Our results demonstrate that the LCA distance is robust to variations in underlying taxonomies or hierarchies. (5) We illustrate the potential of this benchmark by demonstrating how model generalization can be enhanced by aligning model predictions with the class hierarchy. ",
        "bbox": [
            89,
            84,
            473,
            386
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "2. LCA Distance Measures Misprediction Severity ",
        "text_level": 1,
        "bbox": [
            88,
            405,
            434,
            440
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "We propose using the in-distribution Lowest Common Ancestor (LCA) distance, also known as taxonomy loss, as a predictor for model generalization. Here, we formally define how taxonomy loss can be measured using in-distribution data. Taxonomy loss measures the class ranking difference between a model’s prediction based on class likelihood, and a predefined class order encoded by class taxonomy. Lower taxonomy loss is expected when a model assigns higher likelihood to classes that are semantically closer to the ground-truth class, in other words, ‘making better mistakes’ (Bertinetto et al., 2020; Peri et al., 2023). For example, if a cat image is predicted as a dog by model-A and as a car by model-B, model-A would have a lower LCA distance as it makes a better mistake than model-B. Following previous research (Bertinetto et al., 2020; Deng et al., 2009b), we use WordNet (Miller et al., 1990), a large-scale lexical database inspired by psycholinguistic theories of human lexical memory (Miller, 1995), to encode class taxonomy. The WordNet taxonomy is well suited for the widely used ImageNet dataset which builds on WordNet. An example of LCA distance is shown in Fig 3. ",
        "bbox": [
            88,
            449,
            473,
            767
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Given two classes, $y$ (the ground-truth class) and $y ^ { \\prime }$ (the prediction class), we define the LCA distance according to (Bertinetto et al., 2020) as ",
        "bbox": [
            88,
            775,
            473,
            819
        ],
        "page_idx": 2
    },
    {
        "type": "equation",
        "img_path": "images/ac6e1d20228d6ff5a16d9b23758c1292c84f12aed1b4406304ba8dac8d997c3c.jpg",
        "text": "$$\nD _ { L C A } ( y ^ { \\prime } , y ) : = f ( y ) - f ( N _ { L C A } ( y , y ^ { \\prime } ) )\n$$",
        "text_format": "latex",
        "bbox": [
            143,
            830,
            418,
            849
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "where $f ( y ) \\ge f ( N _ { L C A } ( y , y ^ { \\prime } ) )$ and $N _ { L C A } ( y ^ { \\prime } , y )$ denotes the lowest common ancestor class node for classes $y$ and $y ^ { \\prime }$ within the hierarchy, and $f ( \\cdot )$ represents a function of a node, ",
        "bbox": [
            88,
            859,
            473,
            905
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Taxonomydistanceasa measurement of semantic severity of mistake ",
        "bbox": [
            500,
            82,
            879,
            93
        ],
        "page_idx": 2
    },
    {
        "type": "image",
        "img_path": "images/5a147094202fc39b13b675d31e9b918b1b02eaa1cff23705a90f7dee94e5e903.jpg",
        "image_caption": [
            "Figure 3. LCA distance visualization. Our method estimates a model’s generalization based on its in-distribution semantic severity of mistakes. We use the ‘Lowest Common Ancestor’ (LCA) distance to rank the distance between the model’s prediction and the ground-truth class within a predefined taxonomic hierarchy, such as WordNet. The LCA distance is proportional to the shortest path from the prediction to the ground-truth class in the hierarchy. "
        ],
        "image_footnote": [],
        "bbox": [
            503,
            101,
            877,
            270
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "such as the tree depth or entropy. We use the information content as described in (Valmadre, 2022). For each sample $X _ { i }$ in the given dataset $\\mathcal { M } : = X _ { 1 } , \\ldots , X _ { n }$ : ",
        "bbox": [
            500,
            407,
            885,
            452
        ],
        "page_idx": 2
    },
    {
        "type": "equation",
        "img_path": "images/128d24727af963eb5c2063c4643d53bda117ed3e7538c634eb4bea1db2c72a90.jpg",
        "text": "$$\nD _ { L C A } ( m o d e l , \\mathcal { M } ) : = \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } D _ { L C A } ( \\widehat { y } _ { i } , y _ { i } ) \\iff y _ { i } \\neq \\widehat { y } _ { i }\n$$",
        "text_format": "latex",
        "bbox": [
            500,
            460,
            885,
            502
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "where $\\widehat { y } _ { i }$ is the predicted class for sample $X _ { i }$ using the model, $y _ { i }$ is the ground-truth class for sample $X _ { i }$ , and $y _ { i } \\neq { \\widehat { y } } _ { i }$ . Intuitively, a model with a lower LCA distance bdemonstrates a greater semantic understanding of class ontology in WordNet. ",
        "bbox": [
            500,
            510,
            885,
            585
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "We can also derive the generalized form of LCA distance to settings where the model outputs a distribution over all possible classes for each sample (like using softmax), please refer to appendix D.3 for details. ",
        "bbox": [
            500,
            593,
            885,
            654
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "3. Discussion: The Suitability of LCA as a Benchmark for Model Generalization ",
        "text_level": 1,
        "bbox": [
            500,
            672,
            851,
            708
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "This section explores the hypothesis linking LCA distance with a model’s generalization ability and discusses how these insights can be meaningfully and actionably applied. ",
        "bbox": [
            500,
            717,
            883,
            762
        ],
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Our primary motivation is to use class hierarchy to capture correlation invariances across training environments, as proposed in the seminal work on ‘invariant risk minimization’ (Arjovsky et al., 2019). Since the class hierarchy remains consistent across both ID and OOD datasets, it can serve as a surrogate measure of the model’s invariant features. Models that generalize well to OOD datasets typically learn universal or non-spurious features from the training dataset that are transferable to OOD datasets (Makar et al., ",
        "bbox": [
            501,
            770,
            885,
            905
        ],
        "page_idx": 2
    },
    {
        "type": "discarded",
        "text": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies ",
        "bbox": [
            196,
            58,
            776,
            70
        ],
        "page_idx": 2
    },
    {
        "type": "discarded",
        "text": "3 ",
        "bbox": [
            482,
            922,
            491,
            934
        ],
        "page_idx": 2
    },
    {
        "type": "image",
        "img_path": "images/d5579443957d815076e2d61f45629164ffbca6fe51e7b0c82ad3d07d4d6e6bf1.jpg",
        "image_caption": [
            "Figure 4. Capturing transferable features for model generalization. ImageNet-R maintains shape information (Geirhos et al., 2018) like ‘long neck’, ‘big belly’, and ‘long legs’. We hypothesize that models with good generalization should capture these transferable features rather than succumbing to spurious correlations such as ‘grass’, thereby tending to predict classes that are semantically closer to the ground-truth. Such models are expected to have low LCA distances between their predictions and the ground-truth. "
        ],
        "image_footnote": [],
        "bbox": [
            89,
            83,
            470,
            246
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "2022). Such models are more likely to misclassify an ostrich as another bird rather than a lion. These taxonomybased mispredictions, quantified using the LCA distance, are shown to be a better indicator of a model’s OOD performance in this work. ",
        "bbox": [
            89,
            400,
            473,
            474
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Obstacles to Model Generalization. In deep learning, models often learn predictive features from images by creating discriminative associations to class labels. This approach is susceptible to spurious correlations in the training data (Sturm, 2014; Torralba & Efros, 2011; Jabri et al., 2016). For instance, a model might erroneously associate the class ‘ostriches’ with the feature ‘green grass’ in the background, as ostriches often appear in grasslands. These correlations may fail when applied to an OOD dataset that only depicts the semantic concept of ‘ostriches’ (Zhang et al., 2021). ",
        "bbox": [
            88,
            483,
            473,
            633
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Essentials for Model Generalization. ImageNet-R is a severely shifted OOD dataset where, despite significant distribution shifts, humans can effortlessly identify the correct classes. This is because humans can discern stable features across environments. A model’s generalization capability depends on the transferability of the associations learned during training. As benchmarks often simulate humanworld ontology, ideally, only features that align with human understanding of object semantics are universally transferable to any constructed OOD dataset. This underscores the importance of identifying transferable features aligning ontology that contribute to robust model generalization. ",
        "bbox": [
            89,
            641,
            473,
            821
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "How can we measure what features a model has learned as predictive during training? The decision-making process of deep neural networks trained end-to-end has become less interpretable. While there have been attempts to decipher this process by forming decision-tree-like models (Wan et al., 2020; Gare et al., 2022) or through learnable activation functions (Liu et al., 2024), these efforts have not linked this understanding to measure model generalization. ",
        "bbox": [
            89,
            830,
            473,
            905
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            500,
            85,
            885,
            130
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Class Taxonomy Alignment as a Feature Measurement. Class taxonomy or ontology has been widely utilized in literature to indicate class formation (Deng et al., 2009b; Van Horn et al., 2018) and semantic relationships between classes (Frome et al., 2013; Barz & Denzler, 2019; Wan et al., 2020; Redmon & Farhadi, 2017; Lin et al., 2022), offering a hierarchical organization of classes or categories. ",
        "bbox": [
            500,
            137,
            885,
            243
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "As WordNet encodes class ontology, we hypothesize that transferable features are more likely to be shared among neighboring classes in the hierarchy (e.g., ostrich and crane). In contrast, confounding features are less supported by the hierarchy and tend to appear in less relevant classes that are often more distant in the hierarchy (e.g., lion and ostrich). When a model makes a mistake, its secondary prediction class can provide insight into the predictive features the model has learned during training. Specifically, it reflects that the model perceives the label class and the secondary prediction class to be more similar to each other based on these predictive features. ",
        "bbox": [
            501,
            251,
            885,
            431
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Consequently, a model that captures more transferable features tends to ‘make better mistakes (Bertinetto et al., 2020)’ by predicting classes that are semantically closer to the ground-truth class. As illustrated in Fig. 4, models that learns to associate ostriches with features like ‘long legs’ and ‘long neck’, which are more transferable to OOD datasets, will likely predict classes like flamingos or cranes. In contrast, a model influenced by spurious correlations and associating ostriches with grass might predict a semantically distant class, like jaguars or lions, which also often appear on grass. ",
        "bbox": [
            501,
            439,
            885,
            604
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Our method involves measuring model generalization based on the semantic severity of mistakes on in-distribution data. We use the LCA distance, the taxonomic distance between the model’s prediction and the ground-truth class in a predefined taxonomic hierarchy like WordNet. If a model consistently makes better mistakes on in-distribution data, we can reasonably assume that the model has captured more transferable features for class discrimination. ",
        "bbox": [
            500,
            613,
            885,
            733
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Class Taxonomy and Mistake Severity. The severity of a mistake in many studies is quantified as the shortest path from the prediction node to the lowest common ancestor (LCA) node in a predefined class hierarchy. This metric, known as ‘LCA distance’ or ‘hierarchical error’, was used in the early years of the ImageNet challenge (Deng et al., 2009b). However, it was largely dismissed as it was widely believed to follow the same ordering as Top 1 accuracy (Bertinetto et al., 2020). We revisit this metric and empirically demonstrate that Top 1 accuracy and LCA distance do not always align when VLMs are involved, challenging the common notion. We also appeal for community’s attention to revisit this metric with its potential usage in measuring a model’s feature awareness to indicate generalization. ",
        "bbox": [
            501,
            742,
            885,
            892
        ],
        "page_idx": 3
    },
    {
        "type": "discarded",
        "text": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies ",
        "bbox": [
            194,
            58,
            776,
            70
        ],
        "page_idx": 3
    },
    {
        "type": "discarded",
        "text": "4 ",
        "bbox": [
            480,
            924,
            490,
            934
        ],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            88,
            85,
            472,
            145
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Causal/Invariant Representation Learning for OOD Generalization. Recently, there has been an increase in OOD generalization research towards formulating training and testing distributions with causal structures (Arjovsky et al., 2019; Bühlmann, 2020; Peters et al., 2016), where distribution shifts primarily arise from interventions or confounding factors. Building upon this, methods (Schölkopf et al., 2021; Shen et al., 2022; Subramanian et al., 2022) such as CausalVAE (Yang et al., 2021) have been proposed, leveraging learned causal representations to capture the causal relationships underlying the data generation process (Kaur et al., 2022), which helps mitigate the distributional shifts caused by interventions. ",
        "bbox": [
            88,
            154,
            472,
            348
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "While the connection between OOD generalization and causal concepts is not entirely novel, previous attempts have focused on the causal structure at the latent or abstract level, lacking both interpretability and transparency. Our method aligns with this growing interest in causal/invariant learning, which aims to capture the invariant latent data generation process (Kaur et al., 2022). One should expect a model prediction that better aligns with the data generation process to be more robust under intervention, thus generalizing better. Although it is less feasible to model the data generation process of natural images (ImageNet), we essentially follow the same intuition and hypothesize that the WordNet class hierarchy serves as an approximation of invariant correlations between class concepts across environments (Arjovsky et al., 2019; Santurkar et al., 2020), robust to spurious relations in images or shortcuts in learning (Makar et al., 2022). WordNet is a widely recognized and effective means of encoding semantic relationships between concepts, making it an appropriate proxy for aligning human semantic knowledge (Miller et al., 1990). Unlike previous work, WordNet hierarchy provides interpretability, adding a level of transparency to our understanding of model generalization. ",
        "bbox": [
            89,
            357,
            473,
            703
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "LCA Illustration with Simulated Data. To illustrate our hypothesis that LCA distance can identify features supported by hierarchy, we created a controlled example using a simulated dataset, detailed in Appendix C. In this example, the data generation process is fully controlled. We designed a feature space that includes: 1) transferable causal features supported by hierarchy, 2) non-transferable confounding features not supported by hierarchy, and 3) random noise. Two logistic regression models were trained to mimic models capturing different predictive variables from the training data: one relying on the causal features and the other on the confounding features. The simulation results indicated that the model using causal features supported by hierarchy, which exhibited lower LCA distance, had better out-ofdistribution (OOD) accuracy on the in-distribution (ID) test set, despite the model using confounding features achieving better ID accuracy. This example suggests that LCA can effectively identify models that capture relationships aligned with the hierarchical structure. Details in code snippet. ",
        "bbox": [
            89,
            712,
            473,
            892
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            501,
            85,
            885,
            189
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "4. Experiments ",
        "text_level": 1,
        "bbox": [
            501,
            210,
            627,
            227
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "We present experiments benchmarking the relationship between Lowest Common Ancestor (LCA) and generalization. ",
        "bbox": [
            500,
            236,
            887,
            265
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Dataset Setup. We leverage 75 pretrained models sourced from open repositories on GitHub for empirical analysis. Our selection comprises 36 Vision Models (VMs) pretrained on ImageNet and supervised from class labels, alongside 39 Vision-Language Models (VLMs) that incorporate language as part of the supervision. A comprehensive list of model details, ensuring reproducibility, is provided in Appendix A. We use ImageNet (Deng et al., 2009b) as the source indistribution (ID) dataset, while ImageNet- $\\cdot \\nu 2$ (Recht et al., 2019), ImageNet-Sketch (Hendrycks & Dietterich, 2019), ImageNet-Rendition (Hendrycks et al., 2021), ImageNetAdversarial (Hendrycks et al., 2021), and ObjectNets (Barbu et al., 2019) are employed as out-of-distribution datasets, exemplifying severe natural distribution shifts. The ImageNet hierarchy, as depicted in (Bertinetto et al., 2020), is utilized. ",
        "bbox": [
            500,
            285,
            885,
            510
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Although ImageNet-v2 is predominantly deemed an OOD dataset in most prior literature (Shankar et al., 2020; Miller et al., 2021; Baek et al., 2022), our experiments suggest that ImageNet- $\\nu 2$ aligns more closely with ImageNet than other OOD datasets; we delve into these details in Appendix B. ",
        "bbox": [
            500,
            518,
            883,
            593
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Note that the terms in-distribution (ID) and out-ofdistribution (OOD) are not model-specific in this context. Due to the varying distribution of training data across different models, ImageNet may not necessarily represent ID data for models like CLIP, where the training data distribution is not explicitly known. Instead, ID and OOD are relative concepts. ImageNet is used as a reference anchor dataset, serving as a baseline to evaluate the generalization capabilities of models on OOD datasets. This approach aligns with prior work, allowing us to consistently measure the shift in performance from ID to OOD datasets, despite the differences in the training data distributions of the models. ",
        "bbox": [
            501,
            602,
            885,
            781
        ],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Metric Setup. For our correlation experiment, we use $R ^ { 2 }$ (Coefficient of Determination) and PEA (Pearson correlation coefficient) to measure the strength and direction of linear relationships between two variables. Additionally, we employ KEN (Kendall rank correlation coefficient) and $S P E$ (Spearman rank-order correlation coefficient) to assess the correspondence of the rankings of two variables. ",
        "bbox": [
            500,
            800,
            885,
            905
        ],
        "page_idx": 4
    },
    {
        "type": "discarded",
        "text": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies ",
        "bbox": [
            196,
            58,
            776,
            70
        ],
        "page_idx": 4
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            482,
            922,
            490,
            934
        ],
        "page_idx": 4
    },
    {
        "type": "table",
        "img_path": "images/ede3fb3bbb4a1903ec59aa45da9ad51277b45a6d91ef0494b2810da336ce9819.jpg",
        "table_caption": [
            "Table 1. Model performance corresponds to mistake severity. Results are measured by LCA ↓ and Top1 $\\uparrow$ , respectively. indicate measurements on a given dataset. We present model comparisons across VMs and VLMs families. In-distribution LCA distance indicate severely shifted OOD performance (ImageNet-S/R/A/O) better than in-distribution (ImageNet) Top1 accuracy (except for ImageNet-v2). Full 75 models evaluation in Table 2. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Model</td><td colspan=\"2\">ImgN</td><td>ImgN-v2</td><td>ImgN-S</td><td>ImgN-R</td><td>ImgN-A</td><td>ObjNet</td></tr><tr><td></td><td>LCA↓</td><td>Top1↑</td><td>Top1↑</td><td>Top1↑</td><td>Top1↑</td><td>Top1↑</td><td>Top1↑</td></tr><tr><td>ResNet18</td><td>6.643</td><td>0.698</td><td>0.573</td><td>0.202</td><td>0.330</td><td>0.011</td><td>0.272</td></tr><tr><td>ResNet50</td><td>6.539</td><td>0.733</td><td>0.610</td><td>0.235</td><td>0.361</td><td>0.018</td><td>0.316</td></tr><tr><td>CLIP_RN50</td><td>6.327</td><td>0.579</td><td>0.511</td><td>0.332</td><td>0.562</td><td>0.218</td><td>0.398</td></tr><tr><td>CLIP_RN50x4</td><td>6.166</td><td>0.641</td><td>0.573</td><td>0.415</td><td>0.681</td><td>0.384</td><td>0.504</td></tr></table>",
        "bbox": [
            89,
            82,
            473,
            135
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "The importance of these measurements lies in their different focuses. Linearity measures, such as $R ^ { 2 }$ and PEA, are primarily concerned with the fit of a linear model to data points, allowing us to quantify the predictability of changes in one variable based on the other. Ranking measures, like KEN and SPE, provide insights into how the rankings of variables relate to each other, which is crucial in downstream applications such as image retrievals and search engine optimization, where understanding and predicting the ordering of data points is often more important than predicting their exact values. For prediction experiments, we utilize MAE (Mean Absolute Error) to quantify the absolute difference between predictions and ground-truth. ",
        "bbox": [
            89,
            268,
            472,
            464
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "4.1. LCA-on-the-Line: In-Distribution Taxonomic Distance (LCA) as an Out-of-Distribution (OOD) Performance Predictor ",
        "text_level": 1,
        "bbox": [
            88,
            482,
            459,
            526
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Accuracy-on-the-line (Miller et al., 2021) corroborated that a model’s in-distribution (ID) accuracy and its outof-distribution (OOD) accuracy are largely considered to be strongly correlated. This potent correlation forms a significant baseline for comparison in our research. Unlike the framework presented in (Miller et al., 2021), which only compares models within the same modality, our work bridges the gap by contrasting models of different modalities, involving both Vision Models (VM) and VisionLanguage Models (VLM). In addition to the Top1 OOD accuracy, we also incorporate Top5 OOD accuracy, yielding a more comprehensive evaluation of model generalization. ",
        "bbox": [
            89,
            535,
            473,
            715
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "As displayed in Table 1 and 2, the ImageNet in-distribution accuracy (Miller et al., 2021) forms a robust predictor for most OOD datasets, when the comparison is limited to models with similar setups (VMs or VLMs). However, this predictor fails to provide a unified explanation of generalization across models from both families. As highlighted in Figure 5 (indicated in red line), when adhering to Accuracyon-the-Line’ (Miller et al., 2021), all four OOD datasets plotted showcase two separate linear trends, representing models that belong to each family. This observation aligns with (Cherti et al., 2022), where it was found that VLM models, despite exhibiting significantly lower ID accuracy, ",
        "bbox": [
            89,
            724,
            473,
            905
        ],
        "page_idx": 5
    },
    {
        "type": "table",
        "img_path": "images/91989e1ac60cdcd121cfb34ed4ab6f11340e6381cb151f8842f83ac9419be837.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td colspan=\"2\">Element</td><td colspan=\"2\">ImgN-v2</td><td colspan=\"2\">ImgN-S</td><td colspan=\"2\">ImgN-R</td><td colspan=\"2\">ImgN-A</td><td colspan=\"2\">ObjNet</td></tr><tr><td>ID</td><td>0OD</td><td>R²</td><td>PEA</td><td>R</td><td>PEA</td><td>R²</td><td>PEA</td><td>R</td><td>PEA</td><td>R</td><td>PEA</td></tr><tr><td>Top1</td><td>Top1</td><td>0.962</td><td>0.980</td><td>0.075</td><td>0.275</td><td>0.020</td><td>0.140</td><td>0.009</td><td>0.094</td><td>0.273</td><td>0.522</td></tr><tr><td>LCA</td><td>Top1</td><td>0.339</td><td>0.582</td><td>0.816</td><td>0.903</td><td>0.779</td><td>0.883</td><td>0.704</td><td>0.839</td><td>0.915</td><td>0.956</td></tr><tr><td>Top1</td><td>Top5</td><td>0.889</td><td>0.943</td><td>0.052</td><td>0.229</td><td>0.004</td><td>0.060</td><td>0.013</td><td>0.115</td><td>0.262</td><td>0.512</td></tr><tr><td>LCA</td><td>Top5</td><td>0.445</td><td>0.667</td><td>0.811</td><td>0.901</td><td>0.738</td><td>0.859</td><td>0.799</td><td>0.894</td><td>0.924</td><td>0.961</td></tr></table>",
        "bbox": [
            501,
            82,
            883,
            132
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Table 2. Correlation measurement by $R ^ { 2 }$ and $P E A$ of ID LCA/Top1 with OOD Top1/Top5 across 75 models (36 VMs and 39 VLMs) as shown in Figure 5. We demonstrate that LCA has a strong correlation with OOD performance on all listed datasets (except ImageNet-v2). We take the absolute value of all correlations for simplicity. Full table containing results of VMs-only and VLMs-only in Table 11. Measurements from the KEN and SPE show a similar trend as seen in Section $F$ . ",
        "bbox": [
            500,
            136,
            885,
            247
        ],
        "page_idx": 5
    },
    {
        "type": "table",
        "img_path": "images/420a7e82273f23488f8bdeaaf6bb7deb985cfd4f9608b370b5c2415e33079171.jpg",
        "table_caption": [
            "Table 3. Error prediction of OOD datasets across 75 models of diverse settings measured by MAE loss $\\downarrow$ . We mark the best and second best method bold and underline, respectively. Despite ImageNet (ID) accuracy remaining a significant indicator of ImageNet-v2 (OOD) accuracy, the ID LCA serves as a more robust error predictor across the four diverse OOD datasets. Refer to Table 12 for detailed results of VMs-only and VLMs-only. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Methods</td><td>ImgN-v2</td><td>ImgN-S</td><td>ImgN-R</td><td>ImgN-A</td><td>ObjNet</td></tr><tr><td>ID Top1 (Miller et al., 2021)</td><td>0.040</td><td>0.230</td><td>0.277</td><td>0.192</td><td>0.178</td></tr><tr><td>AC (Hendrycks &amp; Gimpel,2017)</td><td>0.043</td><td>0.124</td><td>0.113</td><td>0.324</td><td>0.127</td></tr><tr><td>Aline-D (Baek et al.,2022)</td><td>0.121</td><td>0.270</td><td>0.167</td><td>0.409</td><td>0.265</td></tr><tr><td>Aline-S (Baek et al.,2022)</td><td>0.072</td><td>0.143</td><td>0.201</td><td>0.165</td><td>0.131</td></tr><tr><td>(Ours) ID LCA</td><td>0.162</td><td>0.093</td><td>0.114</td><td>0.103</td><td>0.048</td></tr></table>",
        "bbox": [
            501,
            258,
            883,
            314
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "could attain higher OOD performance than their state-ofthe-art VM counterparts. ",
        "bbox": [
            500,
            443,
            885,
            472
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "As shown in Figure 1, our method, adopting in-distribution LCA distance, could unify models from both families. As demonstrated in Table 2 and Figure 5 (colored in green line), the severity of in-distribution mistakes serves as a more effective indicator of model performance than in-distribution accuracy. It consistently exhibits a strong linear correlation with all OOD benchmark accuracies for natural distribution shifts (both $R ^ { 2 }$ and the Pearson correlation coefficient exceed 0.7, while (Miller et al., 2021) drop to 0 in ImageNetA). Notably, our experiments showed that (Miller et al., 2021) is a more reliable indicator solely for ImageNet-v2, given its visual similarity to ImageNet. We will further discuss this in Appendix B. ",
        "bbox": [
            501,
            481,
            885,
            676
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Our method restores the “on-the-line” linear relationship in front of both VMs and VLMs. Our method provides a compelling alternative to understand why vision-language models with lower in-distribution accuracy might generalize better to OOD datasets than vision models. ",
        "bbox": [
            501,
            685,
            885,
            758
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "4.2. Predicting OOD Performance via ID LCA ",
        "text_level": 1,
        "bbox": [
            500,
            776,
            823,
            791
        ],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "We further highlight the effectiveness of the LCA-on-theLine by estimating model OOD performance using a linear function derived from in-distribution LCA distance. For comparison, we included four competitive baselines: Average Confidence (AC), which leverages OOD logits after temperature scaling; two methods from Agreement-on-theLine (Aline-D and Aline-S), utilizing consensus of pairs of models on OOD benchmarks; and ‘Accuracy on the Line’ (ID Top1), employing in-distribution accuracy of established measurement models to fit a linear function. Instead of performing a probit transform as done in (Baek et al., 2022) and (Miller et al., 2021), we implemented min-max scaling because LCA does not fall within the [0,1] range. ",
        "bbox": [
            501,
            800,
            885,
            905
        ],
        "page_idx": 5
    },
    {
        "type": "discarded",
        "text": "6 ",
        "bbox": [
            480,
            924,
            491,
            934
        ],
        "page_idx": 5
    },
    {
        "type": "discarded",
        "text": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies ",
        "bbox": [
            196,
            58,
            776,
            70
        ],
        "page_idx": 5
    },
    {
        "type": "image",
        "img_path": "images/1baa1a2298a49f38c205b34c061c530b86c803048c124812071859645947cf2a.jpg",
        "image_caption": [
            "Figure 5. Correlating OOD Top-1/Top-5 accuracy $\\mathbf { \\nabla } \\mathbf { V } \\mathbf { M } { + } \\mathbf { V } \\mathbf { L } \\mathbf { M }$ , 75 models) on 4 ImageNet-OOD datasets visualizing Table 2. The plots clearly demonstrate that the in-distribution LCA distance has a stronger correlation with the model’s OOD performance across all OOD datasets than accuracy-on-the-line (Miller et al., 2021). Each plot’s $\\mathbf { X }$ -axis represents the OOD dataset metric (with OOD Top-1 in the top row, and OOD Top-5 accuracy in the bottom row) and $\\mathbf { y }$ -axis represents ImageNet ID test Top-1 accuracy (left) and LCA (right); Red line (Pink dots: VMs and Red dots: VLMs) represents in-distribution classification accuracy (Top-1); Green line (Green dots: VMs and Blue dots: VLMs) denotes in-distribution taxonomic distance (LCA). As interpreted in Figure 1, accuracy-on-the-line only explains generalization of models within similar settings (VMs or VLMs), but does not unify both settings. "
        ],
        "image_footnote": [],
        "bbox": [
            91,
            82,
            885,
            385
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            88,
            525,
            473,
            614
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "As illustrated in Table 3, in-distribution LCA distance proves to be a significantly more robust OOD error predictor than other baselines across four OOD benchmarks with varying distribution shifts. This robustness is especially evident for ImageNet-A, an adversarial dataset derived from ResNet50’s misclassifications on ImageNet. Consequently, models pre-trained on ImageNet tend to underperform on this dataset, especially those with lower accuracy than ResNet50. This leads to decreased robustness for indistribution indicators like in-distribution accuracy (Miller et al., 2021), methods calibrated from in-distribution validation sets (Hendrycks & Gimpel, 2017), and OOD agreement of models from different families (Baek et al., 2022). In contrast, LCA, which relies solely on the relative ranking of class predictions from a single model, is less sensitive to these issues and thus delivers more consistent performance. This further underscores the efficacy of LCA as a powerful predictor in challenging OOD scenarios. ",
        "bbox": [
            88,
            622,
            473,
            893
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "4.3. Enhancing Generalization via Taxonomy Alignment ",
        "text_level": 1,
        "bbox": [
            500,
            525,
            883,
            539
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Building upon the earlier discussion, we explore how the devised method can be utilized to enhance a model’s generalization capability. ",
        "bbox": [
            500,
            547,
            885,
            593
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "4.3.1. INFERRING CLASS TAXONOMY FROM A PRETRAINED MODEL VIA K-MEANS CLUSTERING ",
        "text_level": 1,
        "bbox": [
            500,
            608,
            820,
            652
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "In the previous experiment, we adopted the WordNet hierarchy as class taxonomy to calculate LCA distance. While the number of publicly available datasets providing class taxonomy is limited (Deng et al., 2009b; Van Horn et al., 2018), the usefulness of our method is unquestionable. Hence, we propose a method to construct a latent class taxonomy given a well-trained model on the task, expanding the potential applications of our work. We show that such a constructed taxonomy could achieve similar correlational performance to the WordNet hierarchy. ",
        "bbox": [
            501,
            662,
            885,
            813
        ],
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "The essence of class taxonomy lies in its representation of inter-class distance, encoding class proximity, and identifying which classes cluster closely in feature space. In this spirit, we can construct a class taxonomy matrix using K-means clustering on image features. As illustrated in ",
        "bbox": [
            501,
            820,
            885,
            895
        ],
        "page_idx": 6
    },
    {
        "type": "discarded",
        "text": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies ",
        "bbox": [
            192,
            58,
            777,
            69
        ],
        "page_idx": 6
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            482,
            922,
            490,
            934
        ],
        "page_idx": 6
    },
    {
        "type": "image",
        "img_path": "images/e219173a3ca1459e3aa3be97f30100c204a73ec62f50e3ac94873d6574ee5a25.jpg",
        "image_caption": [
            "Figure 6. Hierarchical structure of image feature clustering using K-means. We construct latent hierarchy through K-means clustering on image features extracted from a pre-trained model. $\\mathrm { K } { = } 1$ represent the most generalized cluster, then we incrementally increase the granularity by splitting into ${ \\tt K } = 2$ and $\\mathrm { K } { = } 4$ clusters. Each node in the hierarchy represents a cluster with the number indicating the class indexes assigned to that cluster. Table 4 show that robust performance can be achieved among 75 latent hierarchy constructed from different pretrained models using clustering. "
        ],
        "image_footnote": [],
        "bbox": [
            91,
            84,
            468,
            247
        ],
        "page_idx": 7
    },
    {
        "type": "table",
        "img_path": "images/63d681f7838ac9c6c6c84cbbc9a1ef8584b7bf5afd6727b45662bf22022af3f8.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=\"2\"></td><td colspan=\"2\">Element</td><td rowspan=\"2\">ImgN-v2</td><td rowspan=\"2\">ImgN-S</td><td rowspan=\"2\">ImgN-R</td><td rowspan=\"2\">ImgN-A</td><td rowspan=\"2\">ObjNet</td></tr><tr><td>ID</td><td>0OD</td></tr><tr><td>Baseline</td><td>Top1</td><td>Top1</td><td>0.980</td><td>0.275</td><td>0.140</td><td>0.094</td><td>0.522</td></tr><tr><td>WordNet</td><td>LCA</td><td>Top1</td><td>0.582</td><td>0.903</td><td>0.883</td><td>0.839</td><td>0.956</td></tr><tr><td>LCA (Statistical Measurements calculated from 75 different Latent Hierarchies)</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Mean</td><td>LCA</td><td>Top1</td><td>0.815</td><td>0.773</td><td>0.712</td><td>0.662</td><td>0.930</td></tr><tr><td>Min</td><td>LCA</td><td>Top1</td><td>0.721</td><td>0.715</td><td>0.646</td><td>0.577</td><td>0.890</td></tr><tr><td>Max</td><td>LCA</td><td>Top1</td><td>0.863</td><td>0.829</td><td>0.780</td><td>0.717</td><td>0.952</td></tr><tr><td>Std</td><td>LCA</td><td>Top1</td><td>0.028</td><td>0.022</td><td>0.027</td><td>0.025</td><td>0.010</td></tr></table>",
        "bbox": [
            88,
            409,
            472,
            494
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Table 4. Correlation measurement $( P E A )$ between ID LCA/Top1 and OOD Top1 across 75 latent hierarchies derived from K-means. Our latent hierarchy construction is robust across 75 different source pretrained models: For each source model, we extracted average class features and applied K-means clustering to construct a latent hierarchy. We then calculated the LCA distance based on each hierarchy, and aggregated the statistical metric of the 75 groups’ Pearson correlation coefficient $( P E A )$ to OOD performance (essentially 75 groups of data from Table 2). $W e$ observe that LCA reliably tracks OOD performance even when using different class taxonomies. ",
        "bbox": [
            88,
            510,
            472,
            661
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Fig. 6, for the ImageNet dataset, we adopt a well-trained model as the source pretrained model and extract average class features to cluster data hierarchically at different levels (we use $\\mathrm { n } { = } 9$ for the 1000-class ImageNet dataset, as $2 ^ { 9 } < 1 0 0 0 \\rangle$ , with an increasing number of clusters to indicate class adjacency. K-mean is performed on each level of hierarchy independently. Experiments in Table 4 show that our method is very robust regardless of which model was used as the source model to construct the class hierarchy. This result demonstrate the potential in practice to use a latent hierarchy constructed by only one well-trained model for evaluating all models on a given task. Further implementation details are provided in Appendix E.1. ",
        "bbox": [
            88,
            709,
            473,
            905
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "4.3.2. USING CLASS TAXONOMY AS SOFT LABELS ",
        "text_level": 1,
        "bbox": [
            500,
            85,
            856,
            99
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "In this section, we investigate how leveraging LCA distance can enhance model generalization through improved supervision. Traditional models maximize the likelihood of the top-1 ground-truth class but often fail to generalize due to overfitting from spurious correlations. We argue that a generalizable model should accurately assign likelihoods to all classes in alignment with the class ontology. Building on this insight, we augment the standard cross-entropy loss, which maximizes the top-1 likelihood, with an auxiliary loss that uses soft labels encoded by the normalized pairwise class distance (LCA distance). This approach treats the problem as multi-label classification (Lin et al., 2022), guiding the model’s decision boundary towards a more regularized feature distribution, thereby reducing susceptibility to spurious correlations and improving generalization. We balance the contributions of the cross-entropy and auxiliary losses using a lambda term: $\\mathrm { L } = \\lambda \\operatorname { L } ( \\mathbf { C } \\mathrm { E } ) + L \\left( s o f t _ { l c a } \\right)$ . The detailed formulation is provided in Appendix E.2. ",
        "bbox": [
            501,
            108,
            885,
            380
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "WordNet as Soft Labels. To evaluate our approach, we trained linear probe layers on five different models using either cross-entropy loss only (Baseline) or our cross-entropy plus LCA soft loss. We compared their performance on six ImageNet test sets. Inspired by the notion that models exhibit higher confidence where they excel (Wortsman et al., 2022), we applied linear interpolation between layers trained with cross-entropy and our proposed loss as our final classifier $W _ { \\mathrm { i n t e r p } } = \\alpha W _ { c e } + ( 1 - \\alpha ) W _ { c e + s o f t } ,$ . Table 5 shows that incorporating LCA soft loss consistently improved OOD performance without compromising ID performance, indicating more regularized decision boundaries beyond training data knowledge. Ablation study is presented in Table 9. ",
        "bbox": [
            501,
            387,
            885,
            583
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Latent Hierarchy as Soft Labels. To demonstrate that our method generalizes beyond WordNet hierarchy, we constructed latent hierarchies using K-means clustering on pretrained models, forming soft labels to guide linear probing. We followed the same training procedure as above, using latent hierarchies instead of WordNet to construct soft labels. As shown in Table 6, adopting constructed hierarchies similarly boosted model generalization across all OOD datasets. ",
        "bbox": [
            501,
            592,
            887,
            712
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "VLMs Construct Better Soft Labels Compared to VMs. Drawing on the intuition of model distillation (Hinton et al., 2015), the hierarchy constructed from a model’s pretrained features partially encapsulates the model’s interpretation of interclass relationships. Thus we also examined if the source model affects the quality of derived soft labels. Figure 7 visualizes pair-wise LCA distance matrices for ImageNet data using hierarchies from different models. ",
        "bbox": [
            501,
            719,
            885,
            840
        ],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "We further conducted a correlation study using latent hierarchies generated from all 75 pretrained models, comparing the source model’s ID LCA evaluated on WordNet, with ",
        "bbox": [
            500,
            848,
            885,
            892
        ],
        "page_idx": 7
    },
    {
        "type": "discarded",
        "text": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies ",
        "bbox": [
            194,
            58,
            776,
            70
        ],
        "page_idx": 7
    },
    {
        "type": "discarded",
        "text": "8 ",
        "bbox": [
            482,
            922,
            491,
            934
        ],
        "page_idx": 7
    },
    {
        "type": "table",
        "img_path": "images/a9805c241e28fb6f3977930a42837207b1cb8b021286ec55ad50df6151ec6736.jpg",
        "table_caption": [
            "Table 5. Soft labeling with WordNet for Linear Probing. Baseline: Trained with Cross Entropy only; Ours: Trained with Cross Entropy $+ \\operatorname { L C A }$ soft loss $^ +$ weight linear interpolation of (CE, CE $^ +$ soft loss) (Wortsman et al., 2022). Results show that integrating soft loss consistently improves model OOD performance, without compromising ID accuracy. Note that in Table 9 of ablation study in pro-OOD setting, we demonstrate that it’s possible to further enhance OOD performance at the cost of a slight ID accuracy drop. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Hierarchy Source:WordNet</td><td colspan=\"2\">ImgNet</td><td colspan=\"2\">ImgNet-V2</td><td colspan=\"2\">ImgNet-S</td><td colspan=\"2\">ImgNet-R</td><td colspan=\"2\">ImgNet-A</td><td colspan=\"2\">ObjectNet</td></tr><tr><td>Backbone Models</td><td>Baseline</td><td>Ours</td><td>Baseline</td><td>Ours</td><td>Baseline</td><td>Ours</td><td>Baseline</td><td>Ours</td><td>Baseline</td><td>Ours</td><td>Baseline</td><td>Ours</td></tr><tr><td>ResNet 18 (He et al.,2016)</td><td>69.4</td><td>69.4 (+0.0)</td><td>56.4</td><td>56.9 (+0.5)</td><td>19.7</td><td>20.7 (+1.0)</td><td>31.9</td><td>33.8 (+1.8)</td><td>1.1</td><td>1.2 (+0.1)</td><td>27.0</td><td>28.0 (+1.0)</td></tr><tr><td>ResNet 50 (He et al., 2016)</td><td>79.5</td><td>79.8 (+0.3)</td><td>67.9</td><td>68.6 (+0.7)</td><td>25.5</td><td>27.7 (+2.2)</td><td>36.5</td><td>42.5 (+6.0)</td><td>10.3</td><td>16.2 (+5.9)</td><td>43.2</td><td>45.5 (+2.3)</td></tr><tr><td>VIT-B (Dosovitskiy et al.,2020)</td><td>75.8</td><td>75.9 (+0.1)</td><td>62.9</td><td>62.8 (-0.1)</td><td>27.0</td><td>27.6 (+0.6)</td><td>40.5</td><td>41.5 (+1.0)</td><td>8.0</td><td>8.6 (+0.6)</td><td>27.6</td><td>28.1(+0.5)</td></tr><tr><td>VIT-L (Dosovitskiy et al.,2020)</td><td>76.8</td><td>76.8(+0.0)</td><td>63.9</td><td>63.8 (-0.1)</td><td>28.4</td><td>29.2 (+0.8)</td><td>42.2</td><td>43.6 (+1.4)</td><td>10.6</td><td>11.5 (+0.9)</td><td>28.7</td><td>29.0 (+0.3)</td></tr><tr><td>ConvNext (Liu etal.,)</td><td>82.0</td><td>82.1 (+0.1)</td><td>70.6</td><td>71.0 (+0.4)</td><td>28.7</td><td>30.0 (+1.3)</td><td>42.4</td><td>44.3 (+1.9)</td><td>21.8</td><td>25.3 (+3.5)</td><td>44.4</td><td>45.5 (+1.1)</td></tr><tr><td>Swin Transformer (Liu et al.,2021)</td><td>83.1</td><td>83.2 (+0.1)</td><td>72.0</td><td>71.9 (-0.1)</td><td>30.3</td><td>31.4 (+1.1)</td><td>43.5</td><td>45.3 (+1.8)</td><td>29.5</td><td>32.7 (+3.2)</td><td>48.3</td><td>49.5 (+1.2)</td></tr></table>",
        "bbox": [
            88,
            82,
            885,
            161
        ],
        "page_idx": 8
    },
    {
        "type": "table",
        "img_path": "images/328ab2eb0fc9983d62be17e187e28499dbee6c5ab70b46361e0a8df99f94eaa9.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td>Backcbone Model:ResNet-18</td><td colspan=\"2\">ImgNet-S</td><td colspan=\"2\">ImgNet-R</td><td colspan=\"2\">ImgNet-A</td><td colspan=\"2\">ObjectNet</td></tr><tr><td>Hierarchy Sources</td><td>Baseline</td><td>Interp</td><td>Baseline</td><td>Interp</td><td>Baseline</td><td>Interp</td><td>Baseline</td><td>Interp</td></tr><tr><td>MnasNet</td><td>19.7</td><td>20.2 (+0.5)</td><td>31.9</td><td>32.4 (+0.5)</td><td>1.1</td><td>1.7 (+0.6)</td><td>27.0</td><td>28.1 (+1.1)</td></tr><tr><td>ResNet 18</td><td>19.7</td><td>20.2 (+0.5)</td><td>31.9</td><td>32.4 (+0.5)</td><td>1.1</td><td>1.8 (+0.7)</td><td>27.0</td><td>28.2 (+1.2)</td></tr><tr><td>vit-1-14</td><td>19.7</td><td>20.8 (+1.2)</td><td>31.9</td><td>33.2 (+1.3)</td><td>1.1</td><td>2.0 (+0.9)</td><td>27.0</td><td>28.3 (+1.3)</td></tr><tr><td>OpenCLIP(vit-1-14)</td><td>19.7</td><td>20.9 (+1.3)</td><td>31.9</td><td>33.7 (+1.8)</td><td>1.1</td><td>2.1 1(+1.0)</td><td>27.0</td><td>28.5 (+1.5)</td></tr><tr><td>WordNet</td><td>19.7</td><td>21.2 (+1.5)</td><td>31.9</td><td>35.1 (+3.2)</td><td>1.1</td><td>1.4 (+0.4)</td><td>27.0</td><td>28.6 (+1.6)</td></tr></table>",
        "bbox": [
            86,
            248,
            473,
            305
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Table 6. Soft Labeling with Latent Hierarchies for Linear Probing on ResNet-18. Instead of using WordNet to construct soft labels in Table 5, we adopted latent hierarchies constructed from pre-trained models using K-means clustering. Results show that using latent hierarchies also delivers a generalization boost compared to the baseline, although it is less significant than using WordNet. Experiments are listed here with the pro-OOD setting in Table 9. ",
        "bbox": [
            88,
            311,
            473,
            407
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "generalization performance from derived soft labels. Table 10 reveals a moderate-strong correlation on ImageNet S/R/A, supported by visualizations in Fig. 8. The findings verify that a latent hierarchy derived from a more generalizable model (aligned closer to the WordNet hierarchy) provides higher quality in guiding the linear probe model training to be more generalizable. This visualization also shows that soft labels constructed from VLMs lead to better generalization. Since soft labels are derived from mean class feature clustering, this suggests that VLMs’ superior generalization may stem from more regularized feature space distributions over encoded class centroids. Future work should explore the reasons behind VLMs’ aligned feature spaces, potentially due to high-level language supervision. ",
        "bbox": [
            89,
            438,
            473,
            647
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "4.3.3. IMPROVING GENERALIZATION BY CLASS TAXONOMY ALIGNMENT WITH PROMPT ENGINEERING ",
        "text_level": 1,
        "bbox": [
            88,
            664,
            419,
            708
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "In this section, we discuss results on enhancing model generalization through prompt engineering in VLMs. ",
        "bbox": [
            88,
            717,
            473,
            747
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "For VLM, integrating taxonomy-specific knowledge during zero-shot evaluation is straightforward. The WordNet hierarchy naturally indicates inter-class distances from class definitions. For example, ‘dalmatian’ and ‘husky’ are semantically close, both originating from the parent node ‘dog’. We detail the results with CLIP-ViT32 (Radford et al., 2021) in Table 14. To test our hypothesis, we explicitly integrated hierarchical taxonomy relationships into the prompt for zero-shot VLM predictions. The prompt was designed as ‘A, which is a type of B, which is a type of ",
        "bbox": [
            89,
            755,
            473,
            905
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "C’, guiding the model to make taxonomy-aligned predictions. Additionally, we conducted two ablation studies: 1) Stack Parent: providing the correct taxonomy path without informing the model of the class name relationships; and 2) Shuffle Parent: informing the model of the hierarchical ‘is-a’ relationship but providing an incorrect taxonomy relationship randomly sampled from the tree. Our results demonstrate that informing the model of both the correct taxonomy and their hierarchical relationships significantly improves generalization. This improvement is evidenced by enhancements in Top-1 accuracy and test-time CrossEntropy (CE) across all datasets for all tested models. ",
        "bbox": [
            500,
            252,
            885,
            433
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "5. Conclusions ",
        "text_level": 1,
        "bbox": [
            501,
            452,
            622,
            468
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "This work revitalizes the use of LCA distance, leveraging class taxonomies such as WordNet, to indicate model OOD performance. We assess the severity of model mispredictions in a manner agnostic to model modality, architecture or training data source, establishing a comprehensive metric for evaluating model generalization. Our findings across multiple ImageNet-OOD datasets highlight the superiority of LCA distance in reflecting the generalization capabilities of models trained with either class labels (VMs) or captions (VLMs), surpassing the traditional reliance on indistribution Top-1 accuracy (Miller et al., 2021). To extend the application of LCA distance measurement to any dataset, we introduce a method for creating latent hierarchies using K-means clustering, showcasing the resilience of LCA distance regardless of the applied taxonomy or hierarchy. Additionally, we demonstrate that aligning model predictions with class taxonomies, through soft labels or prompt engineering, can enhance model generalization. Our results on demonstrating VLMs’ lower LCA distance and better soft label construction offer new insights into VLMs’ superior model generalization from a feature distribution perspective. ",
        "bbox": [
            501,
            478,
            885,
            794
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Future research could focus on providing theoretical justification for the LCA-on-the-Line framework. For instance, exploring causal discovery (Brouillard et al., 2020) methods on the ImageNet dataset to construct a causal graph between classes and underlying variables may offer a more accurate reflection of causal relationships between classes. ",
        "bbox": [
            501,
            803,
            885,
            892
        ],
        "page_idx": 8
    },
    {
        "type": "discarded",
        "text": "9 ",
        "bbox": [
            480,
            922,
            491,
            934
        ],
        "page_idx": 8
    },
    {
        "type": "discarded",
        "text": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies ",
        "bbox": [
            194,
            58,
            777,
            70
        ],
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Acknowledgements ",
        "text_level": 1,
        "bbox": [
            91,
            83,
            251,
            99
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Authors thank Deva Ramanan for insightful discussions, and Hualiang Wang for valuable feedback on the manuscript. The work was partially supported by the CMU Argo Research Center. Shu Kong is partially supported by the University of Macau (SRG2023-00044-FST). ",
        "bbox": [
            89,
            109,
            473,
            184
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Limitation ",
        "text_level": 1,
        "bbox": [
            88,
            204,
            178,
            220
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "While we benchmarked and used LCA based on class hierarchy to measure model generalization, the findings from this work indicate that it is not an effective indicator for datasets visually similar to in-distribution data (like ImageNet2, more discussion in Appendix B). For these datasets, in-distribution Top1 remains a strong indicator, which potentially limits the utility of LCA. Also, it is expected that LCA will show a weaker discrimination between models on datasets with small number of classes (like CIFAR (Krizhevsky et al.)). ",
        "bbox": [
            89,
            231,
            473,
            381
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Impact Statement ",
        "text_level": 1,
        "bbox": [
            89,
            400,
            238,
            417
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "This research aims to enhance our understanding of model generalization mechanisms. However, it is crucial to recognize its potential misuse, such as in guiding adversarial attacks that reduce the generalization capabilities of existing models. Although not the intended purpose of our research, the dual potential of our findings in model generalization underscores the need for robust, secure model development and the implementation of ethical guidelines for deploying this knowledge. ",
        "bbox": [
            89,
            428,
            473,
            561
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "References ",
        "text_level": 1,
        "bbox": [
            89,
            582,
            181,
            598
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Arjovsky, M., Bottou, L., Gulrajani, I., and LopezPaz, D. Invariant risk minimization. arXiv preprint arXiv:1907.02893, 2019.   \nBaek, C., Jiang, Y., Raghunathan, A., and Kolter, J. Z. Agreement-on-the-line: Predicting the performance of neural networks under distribution shift. Advances in Neural Information Processing Systems, 35:19274–19289, 2022.   \nBarbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gutfreund, D., Tenenbaum, J., and Katz, B. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. Advances in neural information processing systems, 32, 2019.   \nBarz, B. and Denzler, J. Hierarchy-based image embeddings for semantic image retrieval. In 2019 IEEE winter conference on applications of computer vision (WACV), pp. 638–647. IEEE, 2019.   \nBertinetto, L., Mueller, R., Tertikas, K., Samangooei, S., and Lord, N. A. Making better mistakes: Leveraging class hierarchies with deep networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12506–12515, 2020.   \nBrouillard, P., Lachapelle, S., Lacoste, A., Lacoste-Julien, S., and Drouin, A. Differentiable causal discovery from interventional data. Advances in Neural Information Processing Systems, 33:21865–21877, 2020.   \nBühlmann, P. Invariance, causality and robustness. 2020.   \nChen, M., Goel, K., Sohoni, N. S., Poms, F., Fatahalian, K., and Ré, C. Mandoline: Model evaluation under distribution shift. In International Conference on Machine Learning, pp. 1617–1629. PMLR, 2021.   \nCherti, M., Beaumont, R., Wightman, R., Wortsman, M., Ilharco, G., Gordon, C., Schuhmann, C., Schmidt, L., and Jitsev, J. Reproducible scaling laws for contrastive language-image learning. arXiv preprint arXiv:2212.07143, 2022.   \nCherti, M., Beaumont, R., Wightman, R., Wortsman, M., Ilharco, G., Gordon, C., Schuhmann, C., Schmidt, L., and Jitsev, J. Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2818–2829, 2023.   \nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009a.   \nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and FeiFei, L. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR, 2009b.   \nDeng, W., Gould, S., and Zheng, L. On the strong correlation between model invariance and generalization. arXiv preprint arXiv:2207.07065, 2022.   \nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.   \nFang, A., Ilharco, G., Wortsman, M., Wan, Y., Shankar, V., Dave, A., and Schmidt, L. Data determines distributional robustness in contrastive language image pre-training (clip). In International Conference on Machine Learning, pp. 6216–6234. PMLR, 2022. ",
        "bbox": [
            86,
            604,
            475,
            905
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "",
        "bbox": [
            496,
            85,
            887,
            909
        ],
        "page_idx": 9
    },
    {
        "type": "discarded",
        "text": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies ",
        "bbox": [
            194,
            58,
            776,
            70
        ],
        "page_idx": 9
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            478,
            922,
            495,
            934
        ],
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Frome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J., Ranzato, M., and Mikolov, T. Devise: A deep visualsemantic embedding model. Advances in neural information processing systems, 26, 2013. ",
        "bbox": [
            88,
            85,
            475,
            145
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Gare, G. R., Fox, T., Lowery, P., Zamora, K., Tran, H. V., Hutchins, L., Montgomery, D., Krishnan, A., Ramanan, D. K., Rodriguez, R. L., et al. Learning generic lung ultrasound biomarkers for decoupling feature extraction from downstream tasks. arXiv preprint arXiv:2206.08398, 2022. ",
        "bbox": [
            88,
            155,
            475,
            244
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F. A., and Brendel, W. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018. ",
        "bbox": [
            88,
            256,
            475,
            330
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "HaoChen, J. Z., Wei, C., Gaidon, A., and Ma, T. Provable guarantees for self-supervised deep learning with spectral contrastive loss. Advances in Neural Information Processing Systems, 34:5000–5011, 2021. ",
        "bbox": [
            88,
            342,
            473,
            401
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In CVPR, 2016. ",
        "bbox": [
            86,
            412,
            473,
            441
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Hendrycks, D. and Dietterich, T. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019. ",
        "bbox": [
            88,
            452,
            473,
            497
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Hendrycks, D. and Gimpel, K. A baseline for detecting misclassified and out-of-distribution examples in neural networks. In ICLR, 2017. ",
        "bbox": [
            88,
            507,
            473,
            553
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai, R., Zhu, T., Parajuli, S., Guo, M., et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8340–8349, 2021. ",
        "bbox": [
            88,
            563,
            475,
            654
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Hinton, G., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. ",
        "bbox": [
            88,
            664,
            473,
            708
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Howard, A., Sandler, M., Chu, G., Chen, L.-C., Chen, B., Tan, M., Wang, W., Zhu, Y., Pang, R., Vasudevan, V., et al. Searching for mobilenetv3. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1314–1324, 2019. ",
        "bbox": [
            88,
            719,
            475,
            795
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Hu, H., Sener, O., Sha, F., and Koltun, V. Drinking from a firehose: Continual learning with web-scale natural language. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. ",
        "bbox": [
            86,
            805,
            473,
            866
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. Densely connected convolutional networks. In ",
        "bbox": [
            88,
            876,
            473,
            905
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700–4708, 2017. ",
        "bbox": [
            514,
            85,
            885,
            114
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Iandola, F. N., Han, S., Moskewicz, M. W., Ashraf, K., Dally, W. J., and Keutzer, K. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5 mb model size. arXiv preprint arXiv:1602.07360, 2016. ",
        "bbox": [
            500,
            126,
            885,
            185
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Jabri, A., Joulin, A., and Van Der Maaten, L. Revisiting visual question answering baselines. In European conference on computer vision, pp. 727–739. Springer, 2016. ",
        "bbox": [
            500,
            196,
            885,
            242
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Jiang, Y., Nagarajan, V., Baek, C., and Kolter, J. Z. Assessing generalization of sgd via disagreement. arXiv preprint arXiv:2106.13799, 2021. ",
        "bbox": [
            500,
            252,
            885,
            297
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Kaur, J. N., Kiciman, E., and Sharma, A. Modeling the datagenerating process is necessary for out-of-distribution generalization. arXiv preprint arXiv:2206.07837, 2022. ",
        "bbox": [
            500,
            308,
            885,
            353
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Krizhevsky, A., Nair, V., and Hinton, G. Cifar-10 (canadian institute for advanced research). URL http://www. cs.toronto.edu/\\~kriz/cifar.html. ",
        "bbox": [
            500,
            364,
            887,
            409
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Krizhevsky, A., Sutskever, I., and Hinton, G. E. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6):84–90, 2017. ",
        "bbox": [
            500,
            420,
            885,
            465
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Larochelle, H., Erhan, D., and Bengio, Y. Zero-data learning of new tasks. In AAAI, volume 1, pp. 3, 2008. ",
        "bbox": [
            500,
            474,
            885,
            506
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Li, C., Zhang, B., Shi, J., and Cheng, G. Multi-level domain adaptation for lane detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4380–4389, 2022a. ",
        "bbox": [
            498,
            516,
            885,
            577
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Li, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., and Hoi, S. C. H. Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems, 34: 9694–9705, 2021. ",
        "bbox": [
            500,
            587,
            885,
            662
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Li, J., Li, D., Xiong, C., and Hoi, S. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, pp. 12888–12900. PMLR, 2022b. ",
        "bbox": [
            500,
            672,
            885,
            747
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Lin, Z., Shi, J., Pathak, D., and Ramanan, D. The clear benchmark: Continual learning on real-world imagery. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. ",
        "bbox": [
            500,
            758,
            887,
            834
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Lin, Z., Pathak, D., Wang, Y.-X., Ramanan, D., and Kong, S. Continual learning with evolving class ontologies. Advances in Neural Information Processing Systems, 35: 7671–7684, 2022. ",
        "bbox": [
            501,
            845,
            887,
            905
        ],
        "page_idx": 10
    },
    {
        "type": "discarded",
        "text": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies ",
        "bbox": [
            192,
            58,
            776,
            70
        ],
        "page_idx": 10
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            478,
            922,
            493,
            935
        ],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 10012–10022, 2021. ",
        "bbox": [
            88,
            85,
            475,
            160
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Liu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., and Xie, S. A convnet for the 2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11976–11986, 2022. ",
        "bbox": [
            86,
            169,
            475,
            229
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Liu, Z., Wang, Y., Vaidya, S., Ruehle, F., Halverson, J., Soljaciˇ c, M., Hou, T. Y., and Tegmark, M. Kan: Kolmogorov- ´ arnold networks. arXiv preprint arXiv:2404.19756, 2024. ",
        "bbox": [
            86,
            238,
            475,
            284
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Lomonaco, V. and Maltoni, D. Core50: a new dataset and benchmark for continuous object recognition. In Conference on Robot Learning, pp. 17–26. PMLR, 2017. ",
        "bbox": [
            88,
            292,
            473,
            338
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Makar, M., Packer, B., Moldovan, D., Blalock, D., Halpern, Y., and D’Amour, A. Causally motivated shortcut removal using auxiliary labels. In International Conference on Artificial Intelligence and Statistics, pp. 739–766. PMLR, 2022. ",
        "bbox": [
            88,
            345,
            475,
            420
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Miller, G. A. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41, 1995. ",
        "bbox": [
            88,
            430,
            475,
            460
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Miller, G. A., Beckwith, R., Fellbaum, C., Gross, D., and Miller, K. J. Introduction to wordnet: An on-line lexical database. International journal of lexicography, 3(4): 235–244, 1990. ",
        "bbox": [
            88,
            469,
            475,
            529
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Miller, J., Krauth, K., Recht, B., and Schmidt, L. The effect of natural distribution shift on question answering models. In International Conference on Machine Learning, pp. 6905–6916. PMLR, 2020. ",
        "bbox": [
            86,
            539,
            475,
            598
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Miller, J. P., Taori, R., Raghunathan, A., Sagawa, S., Koh, P. W., Shankar, V., Liang, P., Carmon, Y., and Schmidt, L. Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization. In International Conference on Machine Learning, pp. 7721– 7735. PMLR, 2021. ",
        "bbox": [
            88,
            607,
            477,
            696
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Mustafa, B., Riquelme, C., Puigcerver, J., Pinto, A. S., Keysers, D., and Houlsby, N. Deep ensembles for lowdata transfer learning. arXiv preprint arXiv:2010.06866, 2020. ",
        "bbox": [
            88,
            707,
            475,
            766
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Peri, N., Dave, A., Ramanan, D., and Kong, S. Towards longtailed 3d detection. In Conference on Robot Learning, 2023. ",
        "bbox": [
            88,
            776,
            475,
            820
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Peters, J., Bühlmann, P., and Meinshausen, N. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal Statistical Society. Series B (Statistical Methodology), pp. 947–1012, 2016. ",
        "bbox": [
            88,
            830,
            475,
            905
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 8748–8763. PMLR, 2021. ",
        "bbox": [
            500,
            85,
            887,
            160
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Radosavovic, I., Kosaraju, R. P., Girshick, R., He, K., and Dollár, P. Designing network design spaces. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 10428–10436, 2020. ",
        "bbox": [
            500,
            169,
            885,
            229
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do imagenet classifiers generalize to imagenet? In International conference on machine learning, pp. 5389–5400. PMLR, 2019. ",
        "bbox": [
            500,
            238,
            885,
            297
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Redmon, J. and Farhadi, A. Yolo9000: better, faster, stronger. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7263–7271, 2017. ",
        "bbox": [
            500,
            306,
            885,
            367
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Roelofs, R., Shankar, V., Recht, B., Fridovich-Keil, S., Hardt, M., Miller, J., and Schmidt, L. A meta-analysis of overfitting in machine learning. Advances in Neural Information Processing Systems, 32, 2019. ",
        "bbox": [
            500,
            376,
            885,
            436
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Santurkar, S., Tsipras, D., and Madry, A. Breeds: Benchmarks for subpopulation shift. arXiv preprint arXiv:2008.04859, 2020. ",
        "bbox": [
            500,
            445,
            885,
            491
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Schölkopf, B., Locatello, F., Bauer, S., Ke, N. R., Kalchbrenner, N., Goyal, A., and Bengio, Y. Toward causal representation learning. Proceedings of the IEEE, 109(5): 612–634, 2021. ",
        "bbox": [
            500,
            500,
            885,
            559
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C., Wightman, R., Cherti, M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022. ",
        "bbox": [
            500,
            569,
            887,
            643
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Shankar, V., Roelofs, R., Mania, H., Fang, A., Recht, B., and Schmidt, L. Evaluating machine accuracy on imagenet. In International Conference on Machine Learning, pp. 8634–8644. PMLR, 2020. ",
        "bbox": [
            500,
            654,
            885,
            713
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Shen, X., Liu, F., Dong, H., Lian, Q., Chen, Z., and Zhang, T. Weakly supervised disentangled generative causal representation learning. Journal of Machine Learning Research, 23:1–55, 2022. ",
        "bbox": [
            498,
            722,
            885,
            781
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Shi, Z., Carlini, N., Balashankar, A., Schmidt, L., Hsieh, C.-J., Beutel, A., and Qin, Y. Effective robustness against natural distribution shifts for models with different training data. arXiv preprint arXiv:2302.01381, 2023. ",
        "bbox": [
            498,
            791,
            885,
            851
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. ",
        "bbox": [
            500,
            861,
            885,
            905
        ],
        "page_idx": 11
    },
    {
        "type": "discarded",
        "text": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies ",
        "bbox": [
            192,
            58,
            776,
            70
        ],
        "page_idx": 11
    },
    {
        "type": "discarded",
        "text": "12 ",
        "bbox": [
            478,
            922,
            495,
            935
        ],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Sturm, B. L. A simple method to determine if a music information retrieval system is a “horse”. IEEE Transactions on Multimedia, 16(6):1636–1644, 2014. ",
        "bbox": [
            88,
            85,
            473,
            128
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Subramanian, J., Annadani, Y., Sheth, I., Ke, N. R., Deleu, T., Bauer, S., Nowrouzezahrai, D., and Kahou, S. E. Learning latent structural causal models. arXiv preprint arXiv:2210.13583, 2022. ",
        "bbox": [
            88,
            140,
            475,
            199
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1–9, 2015. ",
        "bbox": [
            88,
            210,
            475,
            286
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818–2826, 2016. ",
        "bbox": [
            86,
            296,
            475,
            357
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Tan, M. and Le, Q. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pp. 6105–6114. PMLR, 2019. ",
        "bbox": [
            88,
            367,
            475,
            426
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., and Le, Q. V. Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2820–2828, 2019. ",
        "bbox": [
            88,
            438,
            475,
            512
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Taori, R., Dave, A., Shankar, V., Carlini, N., Recht, B., and Schmidt, L. Measuring robustness to natural distribution shifts in image classification. Advances in Neural Information Processing Systems, 33:18583–18599, 2020. ",
        "bbox": [
            88,
            523,
            475,
            583
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Torralba, A. and Efros, A. A. Unbiased look at dataset bias. In CVPR 2011, pp. 1521–1528. IEEE, 2011. ",
        "bbox": [
            88,
            593,
            475,
            623
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Valmadre, J. Hierarchical classification at multiple operating points. arXiv preprint arXiv:2210.10929, 2022. ",
        "bbox": [
            88,
            633,
            473,
            664
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Van Horn, G., Mac Aodha, O., Song, Y., Cui, Y., Sun, C., Shepard, A., Adam, H., Perona, P., and Belongie, S. The inaturalist species classification and detection dataset. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8769–8778, 2018. ",
        "bbox": [
            88,
            674,
            473,
            750
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Wan, A., Dunlap, L., Ho, D., Yin, J., Lee, S., Jin, H., Petryk, S., Bargal, S. A., and Gonzalez, J. E. Nbdt: neural-backed decision trees. arXiv preprint arXiv:2004.00221, 2020. ",
        "bbox": [
            88,
            760,
            475,
            805
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Wortsman, M., Ilharco, G., Kim, J. W., Li, M., Kornblith, S., Roelofs, R., Lopes, R. G., Hajishirzi, H., Farhadi, A., Namkoong, H., et al. Robust fine-tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 7959– 7971, 2022. ",
        "bbox": [
            89,
            814,
            475,
            905
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Yang, M., Liu, F., Chen, Z., Shen, X., Hao, J., and Wang, J. Causalvae: Disentangled representation learning via neural structural causal models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 9593–9602, 2021. ",
        "bbox": [
            500,
            85,
            885,
            160
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Zagoruyko, S. and Komodakis, N. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. ",
        "bbox": [
            498,
            170,
            885,
            200
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107– 115, 2021. ",
        "bbox": [
            500,
            210,
            887,
            270
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "Zhang, X., Zhou, X., Lin, M., and Sun, J. Shufflenet: An extremely efficient convolutional neural network for mobile devices. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6848–6856, 2018. ",
        "bbox": [
            500,
            281,
            887,
            356
        ],
        "page_idx": 12
    },
    {
        "type": "discarded",
        "text": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies ",
        "bbox": [
            194,
            58,
            776,
            70
        ],
        "page_idx": 12
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            478,
            922,
            495,
            934
        ],
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "A. Model Architectures ",
        "text_level": 1,
        "bbox": [
            89,
            83,
            285,
            99
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "We list all models used in ours experiment as follows, including 36 Vision Only Models (VMs) and 39 Vision-Language Models (VLMs). ",
        "bbox": [
            89,
            109,
            880,
            138
        ],
        "page_idx": 13
    },
    {
        "type": "table",
        "img_path": "images/3bd87cc9d9abbdaa86ef789cecb8998da78bc6915823de98f42d146c723abbf2.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=1 colspan=1>ModelCategory</td><td rowspan=1 colspan=1>Architecture</td><td rowspan=1 colspan=1>Number of models</td><td rowspan=1 colspan=1>Checkpoint Link</td></tr><tr><td rowspan=16 colspan=1>VM (Vision-Only-Models)</td><td rowspan=1 colspan=1>AlexNet (Krizhevsky etal.,2017)</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>alexnet</td></tr><tr><td rowspan=1 colspan=1>ConvNeXt (Liu et al.,2022)</td><td rowspan=1 colspan=1>1</td><td rowspan=1 colspan=1>convnext_tiny</td></tr><tr><td rowspan=1 colspan=1>DenseNet (Huang et al.,2017)</td><td rowspan=1 colspan=1>4</td><td rowspan=1 colspan=1>densenet121densenet161densenet169densenet201</td></tr><tr><td rowspan=1 colspan=1>EfficientNet (Tan&amp;Le,2019)</td><td rowspan=1 colspan=1>1</td><td rowspan=1 colspan=1>efficientnet_b0</td></tr><tr><td rowspan=1 colspan=1>GoogLeNett (Szegedy etal.,015)</td><td rowspan=1 colspan=1>1</td><td rowspan=1 colspan=1>googlenet</td></tr><tr><td rowspan=1 colspan=1>Inceptionv3(Szegedy etal.,2016)</td><td rowspan=1 colspan=1>1</td><td rowspan=1 colspan=1>inceptionV3</td></tr><tr><td rowspan=1 colspan=1>MnasNet (Tan et al., 2019)</td><td rowspan=1 colspan=1>4</td><td rowspan=1 colspan=1>mnasnet0.5mnasnet0.75mnasnet1.0mnasnet1.3</td></tr><tr><td rowspan=1 colspan=1>Mobilenet-V3 (Howard et al.,2019)</td><td rowspan=1 colspan=1>2</td><td rowspan=1 colspan=1>mobilenetv3_smallmobilenetv3_large</td></tr><tr><td rowspan=1 colspan=1>Regnet (Radosavovic etal.,2020)</td><td rowspan=1 colspan=1>1</td><td rowspan=1 colspan=1>regnet_y_1_6gf</td></tr><tr><td rowspan=1 colspan=1>Wide ResNet (Zagoruyko&amp;Komodakis,2016)</td><td rowspan=1 colspan=1></td><td rowspan=1 colspan=1>wide_resnet101_2</td></tr><tr><td rowspan=1 colspan=1>ResNet (He et al.,2016)</td><td rowspan=1 colspan=1>5</td><td rowspan=1 colspan=1>resnet18resnet34resnet50resnet101resnet152</td></tr><tr><td rowspan=1 colspan=1>ShuffleNet (Zhang etal.,2018)</td><td rowspan=1 colspan=1>1</td><td rowspan=1 colspan=1>shuflenet_v2_x2_0</td></tr><tr><td rowspan=1 colspan=1>SqueezeNet (Iandola etal.,2016)</td><td rowspan=1 colspan=1>2</td><td rowspan=1 colspan=1>squeezenet1_0squeezenet1_1</td></tr><tr><td rowspan=1 colspan=1>Swin Transformer (Liuet al.,2021)</td><td rowspan=1 colspan=1>1</td><td rowspan=1 colspan=1>swin_b</td></tr><tr><td rowspan=1 colspan=1>VGG (Simonyan &amp; Zisserman, 2015)</td><td rowspan=1 colspan=1>8</td><td rowspan=1 colspan=1>vgg11vgg13vgg16vgg19vgg11_bnvgg13_bnvgg16_bnvgg19_bn</td></tr><tr><td rowspan=1 colspan=1>ViT (Dosovitskiy et al.,2020)</td><td rowspan=1 colspan=1>2</td><td rowspan=1 colspan=1>vit_b_32vit_1_32</td></tr><tr><td rowspan=4 colspan=1>VLM(Vision-Language-Models)</td><td rowspan=1 colspan=1>ALBEF(Li et al.,2021)</td><td rowspan=1 colspan=1>1</td><td rowspan=1 colspan=1>albef_feature_extractor</td></tr><tr><td rowspan=1 colspan=1>BLIP (Li et al., 2022b)</td><td rowspan=1 colspan=1>1</td><td rowspan=1 colspan=1>blip_feature_extractor_base</td></tr><tr><td rowspan=1 colspan=1>CLIP (Radford etal., 2021)</td><td rowspan=1 colspan=1>7</td><td rowspan=1 colspan=1>RN50RN101RN50x4ViT-B-32.ptViT-B-16.ptViT-L-14.ptViT-L-14-336pX</td></tr><tr><td rowspan=1 colspan=1>OpenCLIP(Cherti et al.,2023)</td><td rowspan=1 colspan=1>30</td><td rowspan=1 colspan=1>openCLIP:openCLIP_(&#x27;RN101&#x27;,&#x27;openai&quot;)openCLIP_(RN101&#x27;,&#x27;yfcc15m&quot;)openCLIP_(&#x27;RN101-quickgelu&#x27;,&#x27;openai&#x27;)openCLIP_(&#x27;RN101-quickgelu&#x27;,&#x27;yfcc15m&#x27;)openCLIP_(&#x27;RN50&#x27;,cc12m)openCLIP_(RN50&#x27;,openai)openCLIP_(&#x27;RN50&#x27;,&#x27;yfcc15m)openCLIP_(RN50-quickgelu’,&#x27;cc12m&quot;)OpenCLIP_CRN50-quickgelu’,openai&#x27;)openCLIP_(&#x27;RN50-quickgelu’,&#x27;yfcc15m&#x27;)openCLIP_(RN50x16&#x27;,&#x27;openai&quot;)openCLIP_(RN50x4&#x27;,&#x27;openai&#x27;)openCLIP_(&#x27;RN50x64&#x27;,&#x27;openai&quot;)OpenCLIP_(Vi-B-16’,ion2b_s34b88k)openCLIP_(&#x27;ViT-B-16,&#x27;laion400m_e31&#x27;)openCLIP_(&#x27;ViT-B-16,&#x27;laion400m_e32&quot;)openCLIP_(&#x27;ViT-B-16-plus-240&#x27;,&#x27;laion400m_e31&#x27;)openCLIP_(ViT-B-16-plus-240&#x27;,&#x27;laion400m_e32&quot;)openCLIP_(&#x27;ViT-B-32&#x27;,&#x27;laion2b_e16&#x27;)openCLIP_(&#x27;ViT-B-32&#x27;,&#x27;laion2b_s34b_b79k&#x27;)openCLIP_(Vi-B-32,&#x27;laion400m_e31)OpenCLIP_(ViT-B-32,laion400me32openCLIP_(&#x27;ViT-B-32&#x27;,openai&quot;)openCLIP_(&#x27;ViT-B-32-quickgelu,&#x27;laion400m_e31&#x27;)openCLIP_(&#x27;ViT-B-32-quickgelu,&#x27;laion400m_e32&quot;)openCLIP_(&#x27;ViT-L-14&#x27;,&#x27;laion2b_s32b_b82k&quot;)openCLIP_(&#x27;ViT-L-14&#x27;,&#x27;laion400m_e31&#x27;)openCLIP_(&#x27;ViT-L-14&#x27;,&#x27;laion400m_e32&quot;)openCLIP_(&#x27;coca_ViT-B-32&#x27;,&#x27;laion2b_s13b_b90k&quot;)openCLIP_(&#x27;coca_ViT-L-14&#x27;,&#x27;laion2b_s13b_b90k&quot;)</td></tr></table>",
        "bbox": [
            166,
            150,
            803,
            839
        ],
        "page_idx": 13
    },
    {
        "type": "discarded",
        "text": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies ",
        "bbox": [
            196,
            58,
            776,
            70
        ],
        "page_idx": 13
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            478,
            922,
            495,
            934
        ],
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "B. Discussion ",
        "text_level": 1,
        "bbox": [
            89,
            83,
            200,
            99
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Reestablishing LCA as a Comprehensive Measure of Model Generalization. While Top 1 ID accuracy (Miller et al., 2021) demonstrates a clear linear trend with OOD datasets in models with similar training mechanisms, this relationship becomes less distinct across VMs and VLMs. This finding, echoed in earlier studies (Fang et al., 2022; Wortsman et al., 2022; Cherti et al., 2022), suggests a more nuanced understanding of how zero-shot VLMs with lower Top-1 accuracy can outperform competitive vision models in generalizing to unfamiliar datasets. While previous works have emphasized the significant impact of data diversity on generalization (Fang et al., 2022; Schuhmann et al., 2022; Kaur et al., 2022), our results indicate that the LCA offers a more all-encompassing assessment of model generalization. By considering factors such as training data size, architecture, loss, and others, LCA better measures a model’s ability to accurately capture semantic distinctions common across ID and OOD benchmarks. This establishes a comprehensive benchmark that encompasses various generalization factors, addressing the issue of inflated VLM effectiveness on “Effective Robustness (Taori et al., 2020)”. Future research should delve into large-scale analytic studies of generalization factors in conjunction with LCA. ",
        "bbox": [
            88,
            109,
            885,
            275
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "ImageNet-v2 Demonstrates Similar Class Discrimination Features to ImageNet. ImageNet-v2, a recollection of ImageNet, is often used as an OOD dataset for ImageNet-based studies (Shankar et al., 2020; Miller et al., 2021; Baek et al., 2022). Our experiments indicate that ImageNet-v2 more closely resembles ImageNet than other OOD datasets. We hypothesize that the minimal external intervention in ImageNet-v2’s data collection process results in visual similarities to ImageNet (as ImageNet-v2 is a recollection of ImageNet), allowing even spurious relationships encoded on ImageNet to transfer successfully to ImageNet-v2. Consequently, models pretrained on ImageNet (VMs) inflate accuracy on ImageNet-v2, disrupting the alignment with trends observed in VLMs. ",
        "bbox": [
            88,
            292,
            887,
            398
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Is it Possible for a Semantically-Aware (Low LCA) Model to Have Low Top 1 Accuracy? Our empirical analysis indicates a correlation: models not specifically tuned on class taxonomy, with lower Top 1 accuracy, tend to exhibit higher LCA distances. However, this relationship is correlational rather than causal. It remains feasible to design a model adversarially so it consistently predicts the semantically nearest class to the true class. In such cases, the model would show a low LCA distance while maintaining zero Top 1 accuracy. Therefore, while a correlation exists between Top 1 accuracy and LCA, causality cannot be inferred, and this relationship can be disrupted under deliberate adversarial training. ",
        "bbox": [
            88,
            416,
            885,
            507
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Does ImageNet LCA (Taxonomic Distance) Reflect ImageNet Top 1 Accuracy? It is often suggested that LCA and Top-1 accuracy exhibit similar trends on the same dataset (Deng et al., 2009b; Bertinetto et al., 2020). Intuitively, a high-performing model better fits the data distribution, leading to fewer severe errors. This pattern generally holds true for models under similar settings (either VM or VLM separately). However, when considering both VM and VLM models, ImageNet and ImageNet-v2 exhibit only a weak correlation between LCA and Top-1 accuracy, whereas other semantically distinct OOD datasets show a stronger relationship (validate in Section F.1). This finding challenges the prevailing belief that in-distribution Top-1 accuracy and LCA maintain the same ranking (Deng et al., 2009a; Bertinetto et al., 2020). ",
        "bbox": [
            88,
            525,
            887,
            631
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Why do we observe low LCA correlation numbers between IID test sets? From previous experiments, we observe that ImageNet LCA (Taxonomic Distance) does not correlate strongly with ImageNet/ImageNet-v2 Top-1 Accuracy, often showing a weak relationship, as illustrated in Figure 9. We hypothesize that this is due to ID accuracy inflation. In our LCA-on-the-Line framework, LCA is expected to be an unbiased measure of alignment to the class hierarchy. However, the VMs used in this work are trained on ImageNet and tend to ‘inflate’ ID accuracy when evaluated on IID test sets. As indicated in the bottom right two images of Figure 9, this inflation might causes datapoints to ‘shift’ in the direction of the red arrow, disrupting the unbiased linear relationship seen in VLMs that were not trained directly on ImageNet. Consequently, we should expect models evaluating LCA on unseen datasets to form a linear relationship, similar to the observed relationship on the other four severely shifted OOD datasets in Figure 9. Please refer to Section F.1 and Table 13 for a numerical comparison. ",
        "bbox": [
            88,
            648,
            887,
            799
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "C. LCA Illustration with Simulated Data ",
        "text_level": 1,
        "bbox": [
            89,
            818,
            431,
            835
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "To illustrate the hypotheses in Section 3: 1) Transferable features are more likely to be supported by the hierarchy and shared among neighboring classes; 2) Confounding features are less supported by the hierarchy and tend to appear in less relevant classes that are often more distant in the hierarchy; 3) LCA is useful in identifying features supported by the hierarchy, we created a simple example using a simulated dataset. ",
        "bbox": [
            88,
            844,
            885,
            905
        ],
        "page_idx": 14
    },
    {
        "type": "discarded",
        "text": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies ",
        "bbox": [
            196,
            58,
            776,
            69
        ],
        "page_idx": 14
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            478,
            922,
            495,
            934
        ],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Consider a feature space $\\mathbf { x } : = ( x _ { 1 } , x _ { 2 } , x _ { 3 } ) \\in \\mathbb { R } ^ { 3 }$ and a latent class $z \\in { 1 , 2 , 3 , 4 }$ , where class 1 and 2 are similar, and class 3 and 4 are similar. By design, we set the joint distribution of $\\mathbf { x }$ and $z$ to follow a mixture of Gaussians, where $x _ { 1 } \\in \\{ 1 , 3 , 1 5 , 1 7 \\}$ , $x _ { 2 } \\in \\{ 1 , 1 7 , 7 , 2 1 \\}$ , $x _ { 3 } \\in \\{ 0 , 0 , 0 , 0 \\}$ for each class respectively. ",
        "bbox": [
            88,
            84,
            887,
            131
        ],
        "page_idx": 15
    },
    {
        "type": "equation",
        "img_path": "images/e7b8d126aaa35bf4a5c5d084720f60a8936ee29c1bab2d84605e455a92d067e9.jpg",
        "text": "$$\n\\begin{array} { l l } { { \\mathbf x | z = 1 \\sim N ( \\mu _ { 1 } , \\mathbf I ) , } } & { \\mu _ { 1 } = ( 1 , 1 , 0 ) } \\\\ { { \\mathbf x | z = 2 \\sim N ( \\mu _ { 2 } , \\mathbf I ) , } } & { \\mu _ { 2 } = ( 3 , 1 7 , 0 ) } \\\\ { { \\mathbf x | z = 3 \\sim N ( \\mu _ { 3 } , \\mathbf I ) , } } & { \\mu _ { 3 } = ( 1 5 , 7 , 0 ) } \\\\ { { \\mathbf x | z = 4 \\sim N ( \\mu _ { 4 } , \\mathbf I ) , } } & { \\mu _ { 4 } = ( 1 7 , 2 1 , 0 ) } \\end{array}\n$$",
        "text_format": "latex",
        "bbox": [
            351,
            161,
            619,
            238
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Given a hierarchy preserving class proximity: root : (class 1, class 2), (class 3, class 4), by design, only feature $x _ { 1 }$ supports the class hierarchy, as the distance w.r.t $x _ { 1 }$ between classes 1 & 2 and classes 3 & 4 is smaller than those for other pairs. Feature $x _ { 2 }$ can distinguish all four classes but is not supported by the class hierarchy. Feature $x _ { 3 }$ is random noise with no predictive power for the latent class. ",
        "bbox": [
            88,
            252,
            887,
            313
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "For the in-distribution (ID) data, all three features are observed, while for the out-of-distribution (OOD) data, only $x _ { 1 }$ and $x _ { 3 }$ are observed. From hypothesis in section 3, $x _ { 1 }$ can be considered a transferable causal feature because it is supported by the true class hierarchy and is observable in all datasets. In contrast, $x _ { 2 }$ is a non-transferable confounding feature that does not preserve the class hierarchy and is only observable in the ID data. By design (larger $\\mu$ gap between classes), confounder $x _ { 2 }$ display stronger discrimination among four classes than $x _ { 1 }$ on ID data. ",
        "bbox": [
            88,
            320,
            885,
            396
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "We trained two logistic regression models on the in-distribution (ID) dataset, mimicking models that captured different features as predictive variables learned from the training data. ",
        "bbox": [
            94,
            404,
            883,
            434
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "• Model $f$ , which trains on the transferable causal feature $x _ { 1 }$ , and noise feature $x _ { 3 }$ .   \n• Model $g$ , which trains on the non-transferable confounding feature $x _ { 2 }$ , and noise feature $x _ { 3 }$ . ",
        "bbox": [
            101,
            449,
            725,
            489
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "From simulations (10,000 samples across 100 independent trials), we observed the following results listed in Table 7: ",
        "bbox": [
            93,
            505,
            861,
            520
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "• Model $g$ achieved better ID accuracy because it can leverage $x _ { 2 }$ , which distinguishes all four classes effectively in the ID data.   \n• Model $f$ had better OOD accuracy because $x _ { 1 }$ is a transferable feature that is also present in the OOD data, supported by the true class hierarchy that’s invariant across ID and OOD data.   \n• Model $f$ showed better (lower) LCA distance on the ID test set, indicating that it captures the class hierarchy better by relying on the transferable causal feature $x _ { 1 }$ . ",
        "bbox": [
            106,
            535,
            887,
            645
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "This example illustrates the hypothesis presented in Section 3 and provides the expected output in Table 7. The results suggest that LCA can effectively identify models that capture relationships aligned with the hierarchical structure. For further details, please refer to code snippet. ",
        "bbox": [
            89,
            660,
            888,
            705
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "D. Metric ",
        "text_level": 1,
        "bbox": [
            88,
            724,
            171,
            742
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "In this section, we outline the metrics adopted for our experiment. ",
        "bbox": [
            88,
            751,
            519,
            767
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "D.1. Correlation Measurement ",
        "text_level": 1,
        "bbox": [
            89,
            782,
            303,
            797
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Correlation measurements quantify the degree of association between two variables. This can be further subdivided into linearity and ranking measurements. ",
        "bbox": [
            89,
            806,
            882,
            837
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "D.1.1. LINEARITY MEASUREMENT ",
        "text_level": 1,
        "bbox": [
            89,
            852,
            333,
            866
        ],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Linearity measurement evaluates the strength and direction of a linear relationship between two continuous variables. We use the R² and Pearson correlation coefficients to assess linearity. ",
        "bbox": [
            86,
            875,
            883,
            905
        ],
        "page_idx": 15
    },
    {
        "type": "discarded",
        "text": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies ",
        "bbox": [
            192,
            56,
            777,
            70
        ],
        "page_idx": 15
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            478,
            922,
            495,
            934
        ],
        "page_idx": 15
    },
    {
        "type": "table",
        "img_path": "images/e0ccdb3cb95e8f34abc220b996034935a865f4e843f7ef1413f743be14944a8a.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td></td><td>ID Top1 Error↓</td><td>ID LCA Distance↓</td><td>OOD Top1 Error↓</td></tr><tr><td>g(w. confounding feature)</td><td>0.1423</td><td>2.000</td><td>0.7503</td></tr><tr><td>f(w. transferable feature)</td><td>0.3287</td><td>1.005</td><td>0.3197</td></tr><tr><td>Diff</td><td>+0.1864</td><td>-0.995</td><td>-0.4306</td></tr></table>",
        "bbox": [
            204,
            80,
            764,
            141
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Table 7. Observation from simulation data with 100 trials. The average ID test accuracy error (i.e. top 1 error) ID_Top1_Error $^ { , }$ , ID test LCA distance ID_LCA_Distance $\\downarrow ,$ , and OOD test accuracy error OOD_Top1_Error $\\downarrow$ for generalizable “good” prediction model $f$ and non-generalizable “bad” prediction model $g$ over 100 independent trials. Specifically, we design the data generation process as described in (1), and $f$ is “good” as it learns to rely on the transferable causal features supported by hiearachy; while $g$ is “bad” as it instead relies on the non-transferable confounding features not supported by hiearachy. In this example, ID LCA distance is a better indicator of OOD performance than ID Top1 accuracy, and model f display better generalization to OOD dataset despite lower ID Top 1 accuracy. ",
        "bbox": [
            88,
            151,
            887,
            250
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "$\\mathbf { R } ^ { 2 }$ (Coefficient of determination): The R², or coefficient of determination, quantifies the proportion of the variance in the dependent variable that can be predicted from the independent variable(s). It ranges from 0 to 1, where 1 indicates perfect predictability. It is defined as: ",
        "bbox": [
            86,
            272,
            887,
            316
        ],
        "page_idx": 16
    },
    {
        "type": "equation",
        "img_path": "images/16441b4b481451a69b5dbbd99b4a954e2abc2ba856588da779036fb05624e876.jpg",
        "text": "$$\nR ^ { 2 } = 1 - \\frac { \\sum _ { i = 1 } ^ { n } ( y _ { i } - f ( x _ { i } ) ) ^ { 2 } } { \\sum _ { i = 1 } ^ { n } ( y _ { i } - \\bar { y } ) ^ { 2 } }\n$$",
        "text_format": "latex",
        "bbox": [
            382,
            315,
            591,
            352
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "where $f ( x _ { i } )$ is the prediction of $y _ { i }$ from the model, $\\bar { y }$ is the mean of the actual $y$ values, and $n$ is the number of data points. ",
        "bbox": [
            93,
            356,
            890,
            372
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "PEA (Pearson correlation coefficient): The Pearson correlation coefficient, denoted as $r$ , measures the linear relationship between two datasets. It is defined as: ",
        "bbox": [
            96,
            377,
            885,
            409
        ],
        "page_idx": 16
    },
    {
        "type": "equation",
        "img_path": "images/9eb0fdfb41b05f350c5477e9cf0ff81a140049d3d231e766294b5d32b0488ed5.jpg",
        "text": "$$\nr = { \\frac { \\sum _ { i = 1 } ^ { n } ( x _ { i } - { \\bar { x } } ) ( y _ { i } - { \\bar { y } } ) } { { \\sqrt { \\sum _ { i = 1 } ^ { n } ( x _ { i } - { \\bar { x } } ) ^ { 2 } } } { \\sqrt { \\sum _ { i = 1 } ^ { n } ( y _ { i } - { \\bar { y } } ) ^ { 2 } } } } }\n$$",
        "text_format": "latex",
        "bbox": [
            348,
            415,
            624,
            453
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "where $\\bar { x }$ and $\\bar { y }$ are the mean values of the datasets $x$ and $y$ , respectively, and $n$ is the number of data points. ",
        "bbox": [
            88,
            460,
            784,
            476
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "D.1.2. RANKING MEASUREMENT ",
        "text_level": 1,
        "bbox": [
            89,
            491,
            320,
            505
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Ranking measurement evaluates the degree of correspondence between the rankings of two variables, even when their relationship is non-linear. The Kendall and Spearman rank correlation coefficients are metrics used for this purpose. ",
        "bbox": [
            93,
            513,
            887,
            544
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "KEN (Kendall rank correlation coefficient): Also known as Kendall’s tau $( \\tau )$ , this coefficient measures the ordinal association between two variables. It is defined as: ",
        "bbox": [
            96,
            551,
            883,
            582
        ],
        "page_idx": 16
    },
    {
        "type": "equation",
        "img_path": "images/b5096dfea6b3223ac975ce3798a4fe540839d955844ce0ccdeb8a489ac2b30f3.jpg",
        "text": "$$\n\\tau = \\frac { \\mathrm { ( n u m b e r ~ o f ~ c o n c o r d a n t ~ p a i r s ) - ( n u m b e r ~ o f ~ d i s c o r d a n t ~ p a i r s ) } } { \\frac { 1 } { 2 } n ( n - 1 ) }\n$$",
        "text_format": "latex",
        "bbox": [
            266,
            587,
            705,
            625
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "where $n$ is the number of data points. ",
        "bbox": [
            89,
            632,
            333,
            647
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "SPE (Spearman rank-order correlation coefficient): The Spearman rank-order correlation coefficient, denoted as $\\rho$ assesses the monotonic relationship between two variables. It is defined as: ",
        "bbox": [
            94,
            654,
            887,
            684
        ],
        "page_idx": 16
    },
    {
        "type": "equation",
        "img_path": "images/40bc614802edd9e7ef43cd1772158118e66654e44243e94d2fa341faa10cf227.jpg",
        "text": "$$\n\\rho = 1 - \\frac { 6 \\sum _ { i = 1 } ^ { n } d _ { i } ^ { 2 } } { n ( n ^ { 2 } - 1 ) }\n$$",
        "text_format": "latex",
        "bbox": [
            418,
            693,
            553,
            728
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "where $d _ { i }$ is the difference between the ranks of corresponding data points in the two datasets and $n$ is the number of data points. ",
        "bbox": [
            88,
            736,
            892,
            766
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "D.2. Taxonomy Measurement ",
        "text_level": 1,
        "bbox": [
            88,
            782,
            295,
            797
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Taxonomy measurement is designed to assess the alignment between the model-predicted class ranking and the predefined class taxonomy hierarchy tree. This is also referred to as ‘mistake severity’ or ‘taxonomic distance’. ",
        "bbox": [
            93,
            806,
            885,
            837
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "D.2.1. LCA DISTANCE ",
        "text_level": 1,
        "bbox": [
            88,
            851,
            250,
            866
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Following (Bertinetto et al., 2020; Valmadre, 2022), we define LCA distance using a predefined hierarchy tree, as indicated in Fig. 3. We adopt class distance in a hierarchical tree format to denote inter-class relationships, which is necessary to ",
        "bbox": [
            88,
            875,
            887,
            906
        ],
        "page_idx": 16
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            478,
            922,
            495,
            934
        ],
        "page_idx": 16
    },
    {
        "type": "discarded",
        "text": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies ",
        "bbox": [
            194,
            56,
            776,
            70
        ],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "calculate LCA and ELCA (cf. Section D.3). Given a ground-truth node y (node 1 in the plot), a model prediction node $y ^ { \\prime }$ , and their lowest common ancestor class node $N _ { L C A } ( y , y ^ { \\prime } )$ . We define it as: ",
        "bbox": [
            88,
            84,
            892,
            116
        ],
        "page_idx": 17
    },
    {
        "type": "equation",
        "img_path": "images/b6b88838319b156564b9dc0b7fa44f9eae95cc62b2dcab9cd1977d93d6417f80.jpg",
        "text": "$$\nD _ { L C A } ( y ^ { \\prime } , y ) : = f ( y ) - f ( N _ { L C A } ( y , y ^ { \\prime } ) )\n$$",
        "text_format": "latex",
        "bbox": [
            349,
            127,
            622,
            146
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "where $f ( \\cdot )$ represents a function for a node’s score, such as the tree depth or information content. ",
        "bbox": [
            84,
            159,
            722,
            175
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Scores as tree depths: We define a function $P ( x )$ to retrieve the depth of node $\\mathbf { X }$ from tree T. Then, LCA distance is defined as: ",
        "bbox": [
            88,
            181,
            883,
            212
        ],
        "page_idx": 17
    },
    {
        "type": "equation",
        "img_path": "images/dfc6eecfeb1bb6be07fc704178ac934d32b5a1bed50833a5c9b62c8671bd5736.jpg",
        "text": "$$\nD _ { L C A } ^ { P } ( y ^ { \\prime } , y ) : = ( P ( y ) - P ( N _ { L C A } ( y ^ { \\prime } , y ) ) ) + ( P ( y ^ { \\prime } ) - P ( N _ { L C A } ( y ^ { \\prime } , y ) ) ) ,\n$$",
        "text_format": "latex",
        "bbox": [
            236,
            213,
            735,
            232
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "where we also append $( P ( y ^ { \\prime } ) - P ( N _ { L C A } ( y ^ { \\prime } , y ) ) )$ to counter tree imbalance. ",
        "bbox": [
            86,
            241,
            593,
            258
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Scores as information: Defining score as tree depth may be vulnerable to an imbalanced hierarchical tree. Thus, we also define a node’s score as information to put more weight on nodes with more descendants. Formally, following (Valmadre, 2022), we apply a uniform distribution p to all leaf nodes in the tree that indicate a class in the classification task. The probability of each intermediate node in the tree is calculated by recursively summing the scores of its descendants. Then, the information of each node is calculated as $I ( n o d e ) : = - l o g 2 ( p )$ . The LCA distance is then defined as: ",
        "bbox": [
            88,
            263,
            887,
            340
        ],
        "page_idx": 17
    },
    {
        "type": "equation",
        "img_path": "images/bc0fa05ebe9f81ca6a75b246f57667f67a145f991af91ac7d548a62c4743ab6d.jpg",
        "text": "$$\nD _ { L C A } ^ { I } ( y ^ { \\prime } , y ) : = I ( y ) - I ( N _ { L C A } ( y ^ { \\prime } , y ) ) ,\n$$",
        "text_format": "latex",
        "bbox": [
            348,
            362,
            622,
            382
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "In this work, we adopt $D _ { L C A } ^ { I } ( y ^ { \\prime } , y )$ for LCA measurements, and $D _ { L C A } ^ { P } ( y ^ { \\prime } , y )$ for linear probing experiments. ",
        "bbox": [
            86,
            397,
            805,
            416
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "D.3. ELCA distance ",
        "text_level": 1,
        "bbox": [
            88,
            430,
            230,
            445
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "For a sample $X _ { i }$ whose ground-truth class is $y _ { i }$ , and the model outputs $( \\widehat { p } _ { 1 , i } , \\ldots , \\widehat { p } _ { K , i } )$ over the $K$ classes (e.g., 1000 in b bImageNet), we define the Expected Lowest Common Ancestor Distance (ELCA): ",
        "bbox": [
            93,
            454,
            888,
            484
        ],
        "page_idx": 17
    },
    {
        "type": "equation",
        "img_path": "images/1af896e6a8bf83556971a8d81b2d777867fe02fac2c54add1a0a52fb0adfe56f.jpg",
        "text": "$$\nD _ { E L C A } ( m o d e l , \\mathcal { M } ) : = \\frac { 1 } { n K } \\sum _ { i = 1 } ^ { n } \\sum _ { k = 1 } ^ { K } \\widehat { p } _ { k , i } \\cdot D _ { L C A } ( k , y _ { i } )\n$$",
        "text_format": "latex",
        "bbox": [
            300,
            496,
            673,
            541
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "From a probabilistic perspective, $D _ { E L C A }$ is a weighted measure of mistake severity according to the model’s confidence in each node in the hierarchy. Intuitively, it combines the LCA distance with a cross-entropy measurement. ",
        "bbox": [
            93,
            551,
            887,
            583
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "The proposed ELCA distance provides a more generalized metric for assessing model performance compared to Top 1 accuracy, LCA distance, and cross entropy. Top 1 accuracy only considers the top-ranked class; LCA distance measures the Top n class rankings but treats each class equally (Bertinetto et al., 2020); Cross-entropy solely focuses on the model’s assigned probability to the ground-truth class, and ELCA extends it to all classes. The ELCA distance captures the probabilistic distribution of mistake severity across all candidate classes. ",
        "bbox": [
            88,
            589,
            887,
            665
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "For implementation, ELCA is a weighted combination of the LCA distance for each leaf node [1,2,3,4] as in Fig. 3, weighted by class probability. Formally, for each prediction node $X _ { i }$ , the probabilistic distribution over all candidate classes can be obtained by applying a softmax function $s o f t m a x ( x ) : \\mathbb { R } \\to [ 0 , 1 ]$ to get model outputs probability $( \\widehat { p } _ { 1 , i } , \\ldots , \\widehat { p } _ { K , i } )$ over the $K$ classes (e.g., 1000 in ImageNet). ",
        "bbox": [
            88,
            672,
            885,
            733
        ],
        "page_idx": 17
    },
    {
        "type": "table",
        "img_path": "images/2f7545366482ade4d110c16deaf07dd3c9826ac804ead66beb44febdb58b038d.jpg",
        "table_caption": [
            "In Table 8, we also demonstrate that models with better OOD generalization (OOD Top 1 accuracy) usually also have lower LCA/ELCA distances. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Model</td><td colspan=\"3\">ImageNet</td><td colspan=\"3\">ImageNetv2</td><td colspan=\"3\">ImageNet-S</td><td colspan=\"3\">ImageNet-R</td><td colspan=\"3\"></td><td colspan=\"3\">ImageNet-A</td><td colspan=\"3\">ObjectNet</td></tr><tr><td></td><td>LCA</td><td>ELCA</td><td>Top1</td><td>LCA</td><td></td><td>ELCA</td><td>Top1</td><td>LCA</td><td>ELCA</td><td>Top1</td><td>LCA</td><td>ELCA</td><td>Top1</td><td>LCA</td><td>ELCA</td><td></td><td>Top1</td><td>LCA</td><td>ELCA</td><td>Top1</td></tr><tr><td>ResNet18 (He et al., 2016)</td><td>6.643</td><td>7.505</td><td>0.698</td><td>6.918</td><td>7.912</td><td></td><td>0.573</td><td>8.005</td><td>9.283</td><td>0.202</td><td>8.775</td><td>8.853</td><td>0.330</td><td>8.449</td><td></td><td>9.622</td><td>0.011</td><td>8.062</td><td>8.636</td><td>0.272</td></tr><tr><td>ResNet50 (He et al.,2016)</td><td>6.539</td><td>7.012</td><td>0.733</td><td>6.863</td><td>7.532</td><td>0.610</td><td></td><td>7.902</td><td>9.147</td><td>0.235</td><td>8.779</td><td>8.668</td><td>0.361</td><td>8.424</td><td>9.589</td><td>0.018</td><td></td><td>8.029</td><td>8.402</td><td>0.316</td></tr><tr><td>CLIP_RN50 (Radford et al., 2021)</td><td>6.327</td><td>9.375</td><td>0.579</td><td>6.538</td><td></td><td>9.442</td><td>0.511</td><td>6.775</td><td>9.541</td><td>0.332</td><td>7.764</td><td>9.127</td><td>0.562</td><td>7.861</td><td>9.526</td><td>0.218</td><td></td><td>7.822</td><td>8.655</td><td>0.398</td></tr><tr><td>CLIP_RN50x4 (Radford et al., 2021)</td><td>6.166</td><td>9.473</td><td>0.641</td><td>6.383</td><td></td><td>9.525</td><td>0.573</td><td>6.407</td><td>9.518</td><td>0.415</td><td>7.435</td><td>8.982</td><td>0.681</td><td>7.496</td><td>9.388</td><td></td><td>0.384</td><td>7.729</td><td>8.354</td><td>0.504</td></tr></table>",
        "bbox": [
            89,
            782,
            885,
            835
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Table 8. Model performance corresponds to mistake severity. $\\mathrm { ~  ~ { ~ \\chi ~ } ~ } / \\mathrm { { T o p 1 } } \\uparrow$ indicate measurements on a given dataset. We present two pairs of model comparisons from the VMs and VLMs families with different generalization abilities. Note that ELCA should not be compared across modalities, as it is sensitive to logit temperature. ",
        "bbox": [
            88,
            852,
            887,
            893
        ],
        "page_idx": 17
    },
    {
        "type": "discarded",
        "text": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies ",
        "bbox": [
            191,
            56,
            777,
            70
        ],
        "page_idx": 17
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            478,
            922,
            495,
            934
        ],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "E. Experiment Setup ",
        "text_level": 1,
        "bbox": [
            89,
            83,
            266,
            101
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "E.1. K-mean Clustering for Latent Class Hierarchy Construction ",
        "text_level": 1,
        "bbox": [
            88,
            108,
            540,
            125
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "As depicted in Fig 6, we begin with a pretrained model $M$ , in-distribution image data $X$ , and labels $Y$ for $k$ classes. Initially, we extract the in-distribution data features $M ( X )$ . With known labels, we categorize $M ( X )$ by $Y$ , resulting in $k$ average class features, denoted as $k X$ . Utilizing these per-class average features, we perform a 9-layer hierarchical clustering. For $k X$ , we apply the K-means algorithm, setting the number of cluster centers as $2 ^ { i }$ , where $i$ ranges from $1 , 2 , 3 , 4 , . . . , 9$ since $2 ^ { 9 } < 1 0 0 0$ (ImageNet have 1000 classes). This procedure results in 9 cluster outcomes. Subsequently, we find the LCA node between each pair of the $k$ classes, to determine the cluster level at which both classes exists in the same cluster. We use the height of the common cluster as their pairwise LCA height to be retrieved at training/evaluation. By definition, all classes share a base cluster level of 10. ",
        "bbox": [
            88,
            132,
            885,
            253
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "E.2. Soft Loss for Hierarchy Alignment ",
        "text_level": 1,
        "bbox": [
            89,
            270,
            362,
            285
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "This section illustrates the loss function used in our linear probing experiment. For a dataset with $n$ classes, we first establish an $n \\times n$ LCA distance matrix $M$ (visualize in Figure 7), where $M [ i , k ]$ indicates the pairwise LCA distance $D _ { \\mathrm { L C A } } ( i , k )$ calculated using either the WordNet hierarchy or latent hierarchy derived from the $\\mathbf { K }$ -means clustering (as introduced in the main paper). Next, we scale $M$ by applying a temperature term $T$ , and finally apply MinMax scaling to normalize the values between 0 and 1. ",
        "bbox": [
            88,
            294,
            885,
            369
        ],
        "page_idx": 18
    },
    {
        "type": "equation",
        "img_path": "images/44d01645b1e4908cf2f504f2d921b62584e377a4a6724e069ebe60b1a1ccf7f9.jpg",
        "text": "$$\nM _ { \\mathrm { L C A } } = \\mathbf { M i n M a x } ( M ^ { T } )\n$$",
        "text_format": "latex",
        "bbox": [
            405,
            377,
            568,
            397
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "As shown in the code snippet below, we construct the auxiliary loss by assigning class likelihoods beyond the top-1 (one-hot), extending to all classes. Similar to adopting one-hot encoding to let the model focus on the top-1 ground-truth, we use the reverse of LCA matrix as an alignment indicator, where ground-truth index have the largest value of 1. This alignment can be applied to both BCE and CE types of loss. Details in our code. ",
        "bbox": [
            88,
            421,
            885,
            483
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Algorithm 1 LCA Alignment Loss ",
        "text_level": 1,
        "bbox": [
            88,
            506,
            320,
            521
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "1: function LCA_ALIGNMENT_LOSS(logits, targets, alignment_mode, LCA_matrix, lambda_weight=0.03)   \n2: reverse_LCA_matrix $ 1 -$ LCA_matrix   \n3: Compute predicted probabilities: probs $\\gets$ softmax(logits, dim $^ { 1 = 1 }$ )   \n4: One-hot encode the targets: one_hot_targets   \n5: Compute standard cross-entropy loss:   \nstandard_loss $ - \\sum$ (one_hot_targets $\\cdot$ log(probs), dim $= 1$ )   \n6: if alignment_mode $\\scriptstyle = = \\mathbf { \\nabla } \\cdot _ { \\mathrm { B C E } } ,$ then   \n7: criterion $\\gets$ BCEWithLogitsLoss(reduction $=$ ‘none’)   \n8: Compute soft loss:   \nsoft_loss mean(criterion(logits, reverse_LCA_matrix[targets]), dim $^ { 1 = 1 }$ )   \n9: else if alignment_mode $\\mathrm { \\Phi } = \\mathrm { \\Phi } ^ { \\bullet } \\mathrm { C } \\mathrm { E } ^ { \\prime }$ then   \n10: Compute soft loss:   \nsoft_loss − mean(reverse_LCA_matrix[targets] $\\ast$ log(probs), dim = 1)   \n11: end if   \n12: total_loss $:  \\perp$ ambda_weight · standard_loss $^ +$ soft_loss   \n13: Return mean loss over the batch: return mean(total_loss)   \n14: end function ",
        "bbox": [
            91,
            523,
            848,
            781
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "For the experiments in the main paper, we set lambda $_ { = 0 . 0 3 }$ , temperature $= 2 5$ , and use CE as the soft loss. Note that a smaller lambda scales down the standard cross-entropy loss. We found that using a large temperature, which assign semantic-closer classes with a larger likelihood, boost model generalization better. ",
        "bbox": [
            88,
            804,
            885,
            851
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "E.3. Ablation study: Using class ontology as soft labels ",
        "text_level": 1,
        "bbox": [
            88,
            866,
            478,
            882
        ],
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "In Table 9, we present ablation study on soft loss labels for linear probing from section 4.3.2. ",
        "bbox": [
            89,
            890,
            696,
            906
        ],
        "page_idx": 18
    },
    {
        "type": "discarded",
        "text": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies ",
        "bbox": [
            194,
            58,
            776,
            70
        ],
        "page_idx": 18
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            478,
            922,
            495,
            934
        ],
        "page_idx": 18
    },
    {
        "type": "table",
        "img_path": "images/34eb70224a1609019c5f7a1da034f7b552d71ffbe4baec82866d078ecc3fc80c.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td colspan=\"2\"></td><td>ImgNet</td><td>ImgNet-V2</td><td>ImgNet-S</td><td>ImgNet-R</td><td>ImgNet-A</td><td>ObjectNet</td></tr><tr><td rowspan=\"5\">ResNet 18 (He et al.,2016)</td><td>CE-only</td><td>69.4 69.4</td><td>56.4 56.6</td><td>19.7 19.9</td><td>31.9</td><td>1.1</td><td>27.0 27.4</td></tr><tr><td>CE + interpolation</td><td></td><td></td><td></td><td>32.7</td><td>1.3</td><td></td></tr><tr><td>(Ours) CE + Soft Loss (no ID accuracy drop)</td><td>69.5</td><td>56.5</td><td>19.7</td><td>32.4</td><td>1.1</td><td>27.3</td></tr><tr><td>(Ours) CE + Soft Loss (pro-OOD)</td><td>69.2</td><td>56.4</td><td>20.3</td><td>34.1</td><td>1.4</td><td>27.6</td></tr><tr><td>(Ours) CE + Soft Loss + interpolation (no ID accuracy drop)</td><td>69.4</td><td>56.9</td><td>20.7</td><td>33.8</td><td>1.2</td><td>28.0</td></tr><tr><td rowspan=\"7\">ResNet 50 (He et al.,2016)</td><td>(Ours) CE + Soft Loss + interpolation (pro-OOD)</td><td>68.0</td><td>55.9</td><td>21.2</td><td>35.1</td><td>1.4</td><td>28.6</td></tr><tr><td>CE-only</td><td>79.5</td><td>67.9</td><td>25.5</td><td>36.5</td><td>10.3</td><td>43.2</td></tr><tr><td>CE + interpolation</td><td>79.5</td><td>67.8</td><td>25.6</td><td>36.6</td><td>10.6</td><td>43.3</td></tr><tr><td>(Ours) CE + Soft Loss (no ID accuracy drop)</td><td>79.8</td><td>68.6</td><td>27.7</td><td>42.5</td><td>16.2</td><td>45.5</td></tr><tr><td>(Ours) CE + Soft Loss (pro-OOD)</td><td>79.8</td><td>68.6</td><td>27.7</td><td>42.5</td><td>16.2</td><td>45.5</td></tr><tr><td>(Ours) CE + Soft Loss+ interpolation (no ID accuracy drop)</td><td>79.8</td><td>68.6</td><td>27.7</td><td>42.5</td><td>16.2</td><td>45.5</td></tr><tr><td>(Ours) CE + Soft Loss + interpolation (pro-OOD)</td><td>79.8</td><td>68.6</td><td>27.7</td><td>42.5</td><td>16.2</td><td>45.5</td></tr><tr><td rowspan=\"6\">VIT-B (Dosovitskiy et al.,2020)</td><td>CE-only</td><td>75.8</td><td>62.9</td><td>27.0</td><td>40.5</td><td>8.0</td><td>27.6</td></tr><tr><td>CE+ interpolation</td><td>75.7</td><td>62.4</td><td>27.0</td><td>40.5</td><td>8.2</td><td>27.7</td></tr><tr><td>(Ours) CE + Soft Loss(no ID accuracy drop)</td><td>75.8</td><td>62.7</td><td>26.9</td><td>40.4</td><td>8.2</td><td>27.8</td></tr><tr><td>(Ours) CE + Soft Loss (pro-OOD)</td><td>75.4</td><td>62.4</td><td>28.0</td><td>42.2</td><td>9.1</td><td>27.9</td></tr><tr><td>(Ours) CE + Soft Loss + interpolation (no ID accuracy drop)</td><td>75.9</td><td>62.8</td><td>27.6</td><td>41.5</td><td>8.6</td><td>28.1</td></tr><tr><td>(Ours) CE + Soft Loss + interpolation (pro-OOD)</td><td>75.4</td><td>62.4</td><td>28.0</td><td>42.2</td><td>9.1</td><td>27.9</td></tr><tr><td rowspan=\"6\">VIT-L (Dosovitskiy et al.,2020)</td><td>CE-only</td><td>76.8</td><td>63.9</td><td>28.4</td><td>42.2</td><td>10.6</td><td>28.7</td></tr><tr><td>CE + interpolation</td><td>76.7</td><td>64.0</td><td>28.3</td><td>42.1</td><td>10.9</td><td>28.9</td></tr><tr><td>(Ours) CE + Soft Loss (no ID accuracy drop)</td><td>76.8</td><td>64.1</td><td>28.4</td><td>42.2</td><td>10.5</td><td>28.7</td></tr><tr><td>(Ours) CE + Soft Loss (pro-OOD)</td><td>76.7</td><td>63.6</td><td>29.4</td><td>43.9</td><td>11.7</td><td>29.0</td></tr><tr><td>(Ours) CE + Soft Loss + interpolation (no ID accuracy drop)</td><td>76.8</td><td>63.8</td><td>29.2</td><td>43.6</td><td>11.5</td><td>29.0</td></tr><tr><td>(Ours) CE + Soft Loss + interpolation (pro-OOD)</td><td>76.7</td><td>63.6</td><td>29.4</td><td>43.9</td><td>11.7</td><td>29.0</td></tr><tr><td rowspan=\"6\">ConvNext (Liu et al.,2022)</td><td>CE-only</td><td>82.0</td><td>70.6</td><td>28.7</td><td>42.4</td><td>21.8</td><td>44.4</td></tr><tr><td>CE + interpolation</td><td>82.0</td><td>70.8</td><td>28.8</td><td>42.3</td><td>22.2</td><td>44.7</td></tr><tr><td>(Ours) CE + Soft Loss (no ID accuracy drop)</td><td>82.0</td><td>70.7</td><td>28.7</td><td>42.3</td><td>21.9</td><td>44.6</td></tr><tr><td>(Ours) CE + Soft Loss (pro-OOD)</td><td>81.8</td><td>71.1</td><td>30.4</td><td>44.8</td><td>26.3</td><td>45.7</td></tr><tr><td>(Ours) CE + Soft Loss + interpolation (no ID accuracy drop)</td><td>82.1</td><td>71.0</td><td>30.0</td><td>44.3</td><td>25.2</td><td>45.5</td></tr><tr><td>(Ours) CE + Soft Loss + interpolation (pro-OOD)</td><td>81.8</td><td>71.1</td><td>30.4</td><td>44.8</td><td>26.3</td><td>45.7</td></tr><tr><td rowspan=\"5\">Swin Transformer (Liu et al.,2021)</td><td>CE-only</td><td>83.1</td><td>72.0</td><td>30.3</td><td>43.5</td><td>29.5</td><td>48.3</td></tr><tr><td>CE + interpolation</td><td>83.1</td><td>71.8</td><td>30.4</td><td>43.7</td><td>29.9</td><td>48.3</td></tr><tr><td>(Ours) CE + Soft Loss(no ID accuracy drop)</td><td>83.2</td><td>72.0</td><td>31.0</td><td>44.2</td><td>30.9</td><td>49.0</td></tr><tr><td>(Ours) CE + Soft Loss (pro-OOD)</td><td>83.0</td><td>71.8</td><td>31.6</td><td>45.5</td><td>33.3</td><td>49.4</td></tr><tr><td>(Ours) CE + Soft Loss + interpolation (no ID accuracy drop) (Ours) CE + Soft Loss + interpolation (pro-OOD)</td><td>83.2 83.0</td><td>71.9 71.8</td><td>31.4 31.6</td><td>45.3 45.5</td><td>32.7 33.3</td><td>49.5 49.4</td></tr></table>",
        "bbox": [
            88,
            232,
            887,
            613
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "Table 9. Ablation Study on Soft Loss Labels for Linear Probing from Section 4.3.2. CE-only: model trained with Cross-Entropy (CE) loss only, as a baseline; Soft Loss: soft label loss generated from hierarchy; Interpolation: linear interpolation in weight space between CE-only and the current method; No ID Accuracy Drop: models that do not introduce an accuracy drop on ImageNet (ID) compared to the baseline (CE-only); Pro-OOD: models with parameters that prefer the improvement of OOD generalization, even at the cost of a slight ID accuracy drop, to demonstrate the potential of our methods in enhancing generalization. Note that some models might be selected in multiple settings and appear in multiple rows. Results show that 1). Incorporating soft labels significantly enhances OOD performance across all network architectures without sacrificing ID accuracy. 2). Weight interpolation further boosts OOD generalization, particularly in models supervised with soft labels. 3). Tuning the weight interpolation allows for a balance between maintaining ID accuracy and further improving OOD performance, demonstrating the method’s flexibility and practicality. ",
        "bbox": [
            88,
            628,
            887,
            755
        ],
        "page_idx": 19
    },
    {
        "type": "discarded",
        "text": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies ",
        "bbox": [
            194,
            58,
            776,
            70
        ],
        "page_idx": 19
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            477,
            922,
            496,
            934
        ],
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "E.4. Does the Generalization Quality of the Pretrained Source Model Affect the Quality of Soft Labels? ",
        "text_level": 1,
        "bbox": [
            86,
            84,
            802,
            99
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "This section continues the discussion in Section 4.3.2. We present our findings in Table 10 and Figure 8. The results reveal a moderate-strong correlation between the ID LCA of the pretrained source model, and the generalization capabilities of the linear probe model trained from the source-model-derived latent hierarchy. ",
        "bbox": [
            89,
            108,
            885,
            154
        ],
        "page_idx": 20
    },
    {
        "type": "image",
        "img_path": "images/9ef87c7c0f3589604cd932c556e4dc23dcb59b80f7a94a1829824508df058c5b.jpg",
        "image_caption": [
            "Figure 7. Visualization of pair-wise LCA distance for ImageNet classes. Each row signifies the LCA distance between a specific class and the reference class, arranged in ascending order, with the diagonal index indicating the shortest distance. From left to right: WordNet hierarchy; matrix constructed from ResNet50 (He et al., 2016); and matrix constructed from CLIP ResNet50 (Radford et al., 2021). "
        ],
        "image_footnote": [],
        "bbox": [
            98,
            167,
            875,
            316
        ],
        "page_idx": 20
    },
    {
        "type": "table",
        "img_path": "images/0e3881a58f27bcfcd4dfd7df67f26222b9677235b70cef5382b97409ce96594f.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td></td><td>ImageNet</td><td>ImageNetv2</td><td>ImageNet-S</td><td>ImageNet-R</td><td>ImageNet-A</td><td>ObjectNet</td></tr><tr><td rowspan=\"2\">Corr(ID LCA, Soft Labels Quality</td><td>PEA</td><td>PEA</td><td>PEA</td><td>PEA</td><td>PEA</td><td>PEA</td></tr><tr><td>0.187</td><td>0.301</td><td>0.535</td><td>0.738</td><td>0.604</td><td>0.241</td></tr></table>",
        "bbox": [
            88,
            395,
            885,
            439
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "Table 10. Correlation Measurement between Source Model Generalization Ability and Soft Labels Quality. Following the K-Means clustering algorithm, we constructed 75 LCA distance matrices (class hierarchies) from 75 pretrained source models on ImageNet. We then used these LCA distance matrices as soft labels to guide linear probing over ResNet-18 features (as described in Section 4.3.2). The table indicates a moderate-strong correlation between the in-distribution LCA of the pretrained source model and the out-of-distribution (OOD) accuracy on the linear probe model using the corresponding derived LCA distance matrix. Visualization is shown in Figure 8. ",
        "bbox": [
            88,
            455,
            885,
            525
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "E.5. Hyperparameters and Computational Resources ",
        "text_level": 1,
        "bbox": [
            88,
            546,
            460,
            563
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "In the linear probing experiment, we chose hyperparameters based on the task at hand. The learning rate was set to 0.001, with a batch size of 1024. We used the AdamW optimizer with weight decay and a cosine learning rate scheduler with a warm-up iteration. The warm-up type was set to ‘linear’ with a warm-up learning rate of 1e-5. The experiment was run for 50 epochs. For our computational resources, we utilized a single NVIDIA GeForce GTX 1080 Ti GPU. ",
        "bbox": [
            89,
            570,
            887,
            631
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "F. Supplementary Results ",
        "text_level": 1,
        "bbox": [
            89,
            650,
            305,
            667
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "F.1. Does ImageNet LCA (Taxonomic Distance) Reflect ImageNet Top-1 Accuracy? ",
        "text_level": 1,
        "bbox": [
            89,
            676,
            665,
            691
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "Here, we present numerical results to support the discussion in Section B. We challenge the common belief that LCA and Top-1 accuracy follow parallel trends within the same dataset. As illustrated in Figures 9 and Table 13, when including both VMs and VLMs, ImageNet and ImageNet-v2 show a weak correlation between LCA and Top-1 accuracy within the same dataset. In contrast, other semantically distinct OOD datasets exhibit a stronger relationship. We provide a hypothesis in discussion section B on ‘VMs ID accuracy inflation’ to explain this. ",
        "bbox": [
            88,
            699,
            885,
            775
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "F.2. Comprehensive Results from Main Paper ",
        "text_level": 1,
        "bbox": [
            89,
            787,
            406,
            804
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "Extended from Table 2 and Table 3 in the main paper, we present measurements on only-VMs and only-VLMs in Table 11 and Table 12, respectively. Similarly, LCA is also a very good OOD indicator when involving only VMs or VLMs. ",
        "bbox": [
            93,
            813,
            887,
            842
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "F.3. Ranking Measurement of LCA-on-the-Line ",
        "text_level": 1,
        "bbox": [
            88,
            851,
            421,
            867
        ],
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "Here we present the numerical results for ranking measures in comparison to the commonly used Top-1 in-distribution accuracy in Table 15. Similarly, in-distribution LCA distance presents strong results in both preserving linearity and ranking. ",
        "bbox": [
            89,
            876,
            888,
            905
        ],
        "page_idx": 20
    },
    {
        "type": "discarded",
        "text": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies ",
        "bbox": [
            194,
            58,
            776,
            70
        ],
        "page_idx": 20
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            478,
            922,
            495,
            934
        ],
        "page_idx": 20
    },
    {
        "type": "image",
        "img_path": "images/da265a56478ada65f62ec7713f1c426038a1c3dd67d9864f260c20da4a7bfc57.jpg",
        "image_caption": [
            "Figure 8. Correlation Measurement between Source Model Generalization Ability and Soft Labels Quality. y-axis: LCA distance on ImageNet (ID dataset) between WordNet hierarchy and each of the pretrained models (that generate hierarchies). $4 \\times$ -axis: top-1 accuracy on an OOD dataset by linear probing over each of the generated hierarchies. This plot visualizes the results from Table 10. It shows a moderate-strong correlation between the two variables on ImageNet-S/R/A and ObjectNet(besides some noisy data points). It also indicates that latent hierarchies constructed from VLMs tend to cluster on the right side of the $\\mathbf { X }$ -axis, suggesting better generalization compared to those from VMs. "
        ],
        "image_footnote": [],
        "bbox": [
            91,
            94,
            883,
            486
        ],
        "page_idx": 21
    },
    {
        "type": "table",
        "img_path": "images/6734b4e12612ee63f33ddea882bb3f24b84dc77e053a02ff1a3cbaeac9e2f448.jpg",
        "table_caption": [
            "Table 11. Correlation measurement of ID LCA/Top1 with OOD Top1/Top5 on 75 models across modality following Fig 5. The ‘ALL grouping’ demonstrates that LCA has a strong correlation with OOD performance on all datasets (except ImageNet-v2). We take the absolute value of all correlations for simplicity. Equivalently, LCA is also a very good OOD indicator when only involved VM or VLM. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=\"2\"></td><td colspan=\"2\">Element</td><td colspan=\"2\">ImageNetv2</td><td colspan=\"2\">ImageNet-S</td><td colspan=\"2\">ImageNet-R</td><td colspan=\"2\">ImageNet-A</td><td colspan=\"2\">ObjectNet</td></tr><tr><td>ID</td><td>00D</td><td>R²</td><td>PEA</td><td>R²</td><td>PEA</td><td>R²</td><td>PEA</td><td>R²</td><td>PEA</td><td>R</td><td>PEA</td></tr><tr><td rowspan=\"4\">ALL</td><td>Top1</td><td>Top1</td><td>0.962</td><td>0.980</td><td>0.075</td><td>0.275</td><td>0.020</td><td>0.140</td><td>0.009</td><td>0.094</td><td>0.273</td><td>0.522</td></tr><tr><td>LCA</td><td>Top1</td><td>0.339</td><td>0.582</td><td>0.816</td><td>0.903</td><td>0.779</td><td>0.883</td><td>0.704</td><td>0.839</td><td>0.915</td><td>0.956</td></tr><tr><td>Top1</td><td>Top5</td><td>0.889</td><td>0.943</td><td>0.052</td><td>0.229</td><td>0.004</td><td>0.060</td><td>0.013</td><td>0.115</td><td>0.262</td><td>0.512</td></tr><tr><td>LCA</td><td>Top5</td><td>0.445</td><td>0.667</td><td>0.811</td><td>0.901</td><td>0.738</td><td>0.859</td><td>0.799</td><td>0.894</td><td>0.924</td><td>0.961</td></tr><tr><td rowspan=\"4\">VLM</td><td>Top1</td><td>Top1</td><td>0.996</td><td>0.998</td><td>0.860</td><td>0.927</td><td>0.851</td><td>0.923</td><td>0.578</td><td>0.761</td><td>0.945</td><td>0.972</td></tr><tr><td>LCA</td><td>Top1</td><td>0.956</td><td>0.978</td><td>0.850</td><td>0.921</td><td>0.867</td><td>0.931</td><td>0.691</td><td>0.832</td><td>0.936</td><td>0.968</td></tr><tr><td>Top1</td><td>Top5</td><td>0.988</td><td>0.994</td><td>0.867</td><td>0.931</td><td>0.820</td><td>0.906</td><td>0.740</td><td>0.860</td><td>0.970</td><td>0.985</td></tr><tr><td>LCA</td><td>Top5</td><td>0.930</td><td>0.964</td><td>0.852</td><td>0.923</td><td>0.826</td><td>0.909</td><td>0.822</td><td>0.906</td><td>0.931</td><td>0.965</td></tr><tr><td rowspan=\"4\">VM</td><td>Top1</td><td>Top1</td><td>0.996</td><td>0.998</td><td>0.824</td><td>0.908</td><td>0.801</td><td>0.895</td><td>0.523</td><td>0.723</td><td>0.900</td><td>0.949</td></tr><tr><td>LCA</td><td>Top1</td><td>0.976</td><td>0.988</td><td>0.798</td><td>0.893</td><td>0.768</td><td>0.877</td><td>0.549</td><td>0.741</td><td>0.908</td><td>0.953</td></tr><tr><td>Top1</td><td>Top5</td><td>0.993</td><td>0.997</td><td>0.829</td><td>0.910</td><td>0.821</td><td>0.906</td><td>0.696</td><td>0.834</td><td>0.919</td><td>0.959</td></tr><tr><td>LCA</td><td>Top5</td><td>0.970</td><td>0.985</td><td>0.797</td><td>0.893</td><td>0.777</td><td>0.882</td><td>0.708</td><td>0.841</td><td>0.920</td><td>0.960</td></tr></table>",
        "bbox": [
            86,
            622,
            885,
            835
        ],
        "page_idx": 21
    },
    {
        "type": "discarded",
        "text": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies ",
        "bbox": [
            196,
            58,
            776,
            70
        ],
        "page_idx": 21
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            477,
            922,
            495,
            934
        ],
        "page_idx": 21
    },
    {
        "type": "image",
        "img_path": "images/4117a00fe8e53989d7579f3f11361e1263ba08c21ed68791f00e1f0c98aaec48.jpg",
        "image_caption": [
            "Figure 9. Predicting LCA $\\mathbf { V M + V L M }$ , 75 models) on the same dataset As per Table 13. Each plot’s x-axis represents dataset Top-1 accuracy, while the y-axis shows LCA distance measured on the same datasets. The plots reveal that ImageNet and ImageNet-v2 do not exhibit a strong correlation between LCA and Top-1 accuracy, in contrast to other semantically distinct OOD datasets. This observation challenges the common belief that in-distribution Top-1 accuracy and LCA distance maintain the same order (Deng et al., $2 0 0 9 \\mathrm { a }$ ; Bertinetto et al., 2020). More details in discussion section B. "
        ],
        "image_footnote": [],
        "bbox": [
            94,
            93,
            880,
            487
        ],
        "page_idx": 22
    },
    {
        "type": "table",
        "img_path": "images/32c7b09ad1384a4e5c18d6d1f5cf4f11f0015114b9eee7e8919727b168be91c9.jpg",
        "table_caption": [
            "Table 12. Error Prediction of OOD Datasets across 75 models of diverse settings with MAE loss ↓. Top1 in bold and Top2 in underline. Despite ImageNet’s in-distribution accuracy maintain as a significant indicator of ImageNet-v2 accuracy, the in-distribution LCA outperforms it as a robust error predictor across four naturally distributed OOD datasets. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td colspan=\"2\"></td><td>ImageNetv2</td><td>ImageNet-S</td><td>ImageNet-R</td><td>ImageNet-A</td><td>ObjectNet</td></tr><tr><td rowspan=\"5\">ALL</td><td>ID Top1 (Miller et al., 2021)</td><td>0.040</td><td>0.230</td><td>0.277</td><td>0.192</td><td>0.178</td></tr><tr><td>AC (Hendrycks &amp; Gimpel, 2017)</td><td>0.043</td><td>0.124</td><td>0.113</td><td>0.324</td><td>0.127</td></tr><tr><td>Aline-D (Baek et al., 2022)</td><td>0.121</td><td>0.270</td><td>0.167</td><td>0.409</td><td>0.265</td></tr><tr><td>Aline-S (Baek et al., 2022)</td><td>0.072</td><td>0.143</td><td>0.201</td><td>0.165</td><td>0.131</td></tr><tr><td>(Ours) ID LCA</td><td>0.162</td><td>0.093</td><td>0.114</td><td>0.103</td><td>0.048</td></tr><tr><td rowspan=\"5\">VLM</td><td>ID (Miller et al., 2021)</td><td>0.014</td><td>0.077</td><td>0.064</td><td>0.127</td><td>0.052</td></tr><tr><td>AC (Hendrycks &amp; Gimpel, 2017)</td><td>0.029</td><td>0.050</td><td>0.044</td><td>0.217</td><td>0.088</td></tr><tr><td>Aline-D (Baek et al., 2022)</td><td>0.151</td><td>0.250</td><td>0.081</td><td>0.296</td><td>0.260</td></tr><tr><td>Aline-S (Baek et al., 2022)</td><td>0.070</td><td>0.069</td><td>0.068</td><td>0.080</td><td>0.153</td></tr><tr><td>(Ours) ID LCA</td><td>0.047</td><td>0.083</td><td>0.070</td><td>0.105</td><td>0.043</td></tr><tr><td rowspan=\"5\">VM</td><td>ID (Miller et al., 2021)</td><td>0.013</td><td>0.099</td><td>0.108</td><td>0.143</td><td>0.068</td></tr><tr><td>AC (Hendrycks &amp; Gimpel,2017)</td><td>0.059</td><td>0.204</td><td>0.188</td><td>0.441</td><td>0.168</td></tr><tr><td>Aline-D (Baek et al., 2022)</td><td>0.083</td><td>0.427</td><td>0.313</td><td>0.665</td><td>0.364</td></tr><tr><td>Aline-S (Baek et al., 2022)</td><td>0.105</td><td>0.182</td><td>0.092</td><td>0.574</td><td>0.216</td></tr><tr><td>(Ours) ID LCA</td><td>0.029</td><td>0.102</td><td>0.113</td><td>0.145</td><td>0.065</td></tr></table>",
        "bbox": [
            88,
            611,
            883,
            835
        ],
        "page_idx": 22
    },
    {
        "type": "discarded",
        "text": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies ",
        "bbox": [
            194,
            58,
            776,
            70
        ],
        "page_idx": 22
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            478,
            922,
            495,
            934
        ],
        "page_idx": 22
    },
    {
        "type": "table",
        "img_path": "images/cffcc18d9e2156c40eda56ee41e13252fa0e3a879934c7d81fb25048f54fc07b.jpg",
        "table_caption": [
            "Table 13. Correlation Measurement between Top-1 Accuracy and LCA on the Same Dataset. This analysis uses 75 models across different modalities (36 VMs and 39 VLMs) on all six ImageNet datasets. While the main paper employs ID LCA to predict OOD performance (e.g., Corr(ImageNet LCA, ImageNet-A Top-1 Accuracy)), this setting differs by using LCA to predict Top-1 accuracy on the same dataset (e.g., Corr(ImageNet-A LCA, ImageNet-A Top-1 Accuracy)). Following Figure 9, we highlight strong correlation indications. For simplicity, we take the absolute value of all correlations. More details in discussion section B. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Model</td><td>Group</td><td colspan=\"2\">ImageNet</td><td colspan=\"2\">ImageNetv2</td><td colspan=\"2\">ImageNet-S</td><td colspan=\"2\">ImageNet-R</td><td colspan=\"2\">ImageNet-A</td><td colspan=\"2\">ObjectNet</td></tr><tr><td rowspan=\"4\">Top1-&gt;LCA</td><td>ALL</td><td>R 0.174 KEN 0.280</td><td>PEA 0.417 SPE 0.266</td><td>R 0.114 KEN 0.237</td><td>PEA 0.337 SPE 0.294</td><td>R² 0.835 KEN 0.818</td><td>PEA 0.914 SPE 0.926</td><td>R 0.770 KEN 0.621</td><td>PEA 0.878 SPE 0.803</td><td>R² 0.851 KEN 0.825</td><td>PEA 0.923 SPE 0.951</td><td>R 0.657 KEN 0.673</td><td>PEA 0.810 SPE</td></tr><tr><td>VLM</td><td>R 0.938 KEN</td><td>PEA 0.969 SPE 0.969</td><td>R 0.891 KEN 0.799</td><td>PEA 0.944 SPE</td><td>R 0.945 KEN</td><td>PEA 0.972 SPE</td><td>R 0.878 KEN</td><td>PEA 0.937 SPE</td><td>R 0.725 KEN</td><td>PEA 0.851 SPE</td><td>R 0.510 KEN</td><td>0.823 PEA 0.714 SPE</td></tr><tr><td>VM</td><td>0.880 R 0.973 KEN 0.911</td><td>PEA 0.986 SPE 0.980</td><td>R 0.890 KEN 0.758</td><td>0.881 PEA 0.943 SPE 0.910</td><td>0.864 R 0.934 KEN 0.854</td><td>0.963 PEA 0.966 SPE 0.963</td><td>0.753 R 0.095 KEN 0.149</td><td>0.902 PEA 0.310 SPE 0.222</td><td>0.689 R 0.840 KEN 0.839</td><td>0.869 PEA 0.916 SPE 0.952</td><td>0.529 R 0.948 KEN 0.854</td><td>0.720 PEA 0.974 SPE 0.960</td></tr></table>",
        "bbox": [
            89,
            107,
            883,
            281
        ],
        "page_idx": 23
    },
    {
        "type": "table",
        "img_path": "images/bf5ef7fe5d556e888492a9e0940e9fcd9115e6605899c1df8cb2ce6b30f49b1d.jpg",
        "table_caption": [
            "Table 14. Accuracy on OOD dataset by enforcing class taxonomy: Baseline: <dalmatian>; Stack Parent: <dalmatian, dog, animal>; Taxonomy Parent:<dalmatian, which is type of a dog, which is type of an animal>; Shuffle Parent: <dalmatian, which is type of an organism, which is type of a seabird>; The Taxonomy Parent method, which includes the full hierarchical relationship, yields the best performance, highlighting the effectiveness of incorporating structured knowledge into model predictions. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=\"2\">Model</td><td colspan=\"2\">ImgN</td><td colspan=\"2\">ImgN-v2</td><td colspan=\"2\">ImgN-S</td><td colspan=\"2\">ImgN-R</td><td colspan=\"2\">ImgN-A</td><td colspan=\"2\">ObjNet</td></tr><tr><td>Top1个</td><td>Test CE</td><td>Top1个</td><td>Test CE</td><td>Top1个</td><td>TestCE</td><td>Top1个</td><td>Test CE↓</td><td>Top1个</td><td>Test CE</td><td>Top1个</td><td>Test CE↓</td></tr><tr><td>Baseline</td><td>0.589</td><td>9.322</td><td>0.517</td><td>9.384</td><td>0.379</td><td>9.378</td><td>0.667</td><td>8.790</td><td>0.294</td><td>9.358</td><td>0.394</td><td>8.576</td></tr><tr><td>Stack Parent</td><td>0.381</td><td>9.389</td><td>0.347</td><td>9.395</td><td>0.219</td><td>9.561</td><td>0.438</td><td>9.258</td><td>0.223</td><td>9.364</td><td>0.148</td><td>9.076</td></tr><tr><td>Shuffle Parent</td><td>0.483</td><td>9.679</td><td>0.432</td><td>9.696</td><td>0.329</td><td>9.718</td><td>0.557</td><td>9.281</td><td>0.236</td><td>9.586</td><td>0.329</td><td>8.785</td></tr><tr><td>Taxonomy Parent</td><td>0.626</td><td>9.102</td><td>0.553</td><td>9.165</td><td>0.419</td><td>9.319</td><td>0.685</td><td>8.658</td><td>0.319</td><td>9.171</td><td>0.431</td><td>8.515</td></tr></table>",
        "bbox": [
            89,
            425,
            856,
            488
        ],
        "page_idx": 23
    },
    {
        "type": "table",
        "img_path": "images/250d878dd8ef7e6986b593112a71355fa1826d43831aa3fb7a12b930dd8e9fed.jpg",
        "table_caption": [
            "Table 15. Ranking measurement of ID LCA/Top1 with OOD Top1/Top5 on 75 models across modality(36 VMs and 39 VLMs); As shown in the ‘ALL grouping’, LCA shows a much better result in preserving the model relative ranking to model OOD performance on all OOD datasets (with the exception of ImageNet-v2), which indicates its superiority for model selection. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=\"2\"></td><td colspan=\"2\">Element</td><td colspan=\"2\">ImageNetv2</td><td colspan=\"2\">ImageNet-S</td><td colspan=\"2\">ImageNet-R</td><td colspan=\"2\">ImageNet-A</td><td colspan=\"2\">ObjectNet</td></tr><tr><td>ID</td><td>00D</td><td>KEN</td><td>SPE</td><td>KEN</td><td>SPE</td><td>KEN</td><td>SPE</td><td>KEN</td><td>SPE</td><td>KEN</td><td>SPE</td></tr><tr><td rowspan=\"4\">ALL</td><td>Top1</td><td>Top1</td><td>0.840</td><td>0.947</td><td>0.170</td><td>0.092</td><td>0.146</td><td>0.042</td><td>0.068</td><td>0.037</td><td>0.317</td><td>0.339</td></tr><tr><td>LCA</td><td>Top1</td><td>0.421</td><td>0.517</td><td>0.779</td><td>0.923</td><td>0.761</td><td>0.911</td><td>0.730</td><td>0.888</td><td>0.867</td><td>0.967</td></tr><tr><td>Top1</td><td>Top5</td><td>0.672</td><td>0.818</td><td>0.151</td><td>0.059</td><td>0.134</td><td>0.004</td><td>0.108</td><td>0.021</td><td>0.279</td><td>0.297</td></tr><tr><td>LCA</td><td>Top5</td><td>0.571</td><td>0.729</td><td>0.768</td><td>0.919</td><td>0.752</td><td>0.897</td><td>0.755</td><td>0.908</td><td>0.861</td><td>0.966</td></tr><tr><td rowspan=\"4\">VLM</td><td>Top1</td><td>Top1</td><td>0.971</td><td>0.997</td><td>0.840</td><td>0.936</td><td>0.864</td><td>0.943</td><td>0.753</td><td>0.915</td><td>0.905</td><td>0.982</td></tr><tr><td>LCA</td><td>Top1</td><td>0.882</td><td>0.972</td><td>0.729</td><td>0.861</td><td>0.762</td><td>0.886</td><td>0.800</td><td>0.942</td><td>0.870</td><td>0.972</td></tr><tr><td>Top1</td><td>Top5</td><td>0.908</td><td>0.980</td><td>0.848</td><td>0.951</td><td>0.882</td><td>0.959</td><td>0.753</td><td>0.910</td><td>0.842</td><td>0.964</td></tr><tr><td>LCA</td><td>Top5</td><td>0.900</td><td>0.981</td><td>0.746</td><td>0.879</td><td>0.775</td><td>0.907</td><td>0.794</td><td>0.943</td><td>0.829</td><td>0.955</td></tr><tr><td rowspan=\"4\">VM</td><td>Top1</td><td>Top1</td><td>0.948</td><td>0.993</td><td>0.771</td><td>0.901</td><td>0.743</td><td>0.887</td><td>0.735</td><td>0.877</td><td>0.822</td><td>0.927</td></tr><tr><td>LCA</td><td>Top1</td><td>0.910</td><td>0.981</td><td>0.740</td><td>0.882</td><td>0.705</td><td>0.862</td><td>0.741</td><td>0.851</td><td>0.790</td><td>0.918</td></tr><tr><td>Top1</td><td>Top5</td><td>0.939</td><td>0.992</td><td>0.752</td><td>0.894</td><td>0.758</td><td>0.901</td><td>0.818</td><td>0.941</td><td>0.815</td><td>0.920</td></tr><tr><td>LCA</td><td>Top5</td><td>0.894</td><td>0.977</td><td>0.733</td><td>0.879</td><td>0.707</td><td>0.871</td><td>0.780</td><td>0.916</td><td>0.783</td><td>0.911</td></tr></table>",
        "bbox": [
            86,
            607,
            885,
            819
        ],
        "page_idx": 23
    },
    {
        "type": "discarded",
        "text": "LCA-on-the-Line: Benchmarking Out of Distribution Generalization with Class Taxonomies ",
        "bbox": [
            194,
            58,
            776,
            70
        ],
        "page_idx": 23
    },
    {
        "type": "discarded",
        "text": "",
        "bbox": [
            477,
            922,
            495,
            934
        ],
        "page_idx": 23
    }
]